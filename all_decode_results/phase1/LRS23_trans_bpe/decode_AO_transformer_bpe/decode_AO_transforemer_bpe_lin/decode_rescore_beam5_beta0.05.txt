2023-02-05 11:16:03,374 - __main__ - INFO - Namespace(batch_size=48, beam=500, beamWidth=5, beamsizetoken=None, beamthreshold=100.0, beta=0.05, decode_type='HYBRID_RESCORE', eval_lrs3_model_file='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_0604-wer_0.054.ckpt', lexicon='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/lst/LRS23.lst', lmpath='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/LRS23_4gram.bin', lmweight=1, logname='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main_noneed/decode_AO_transformer_bpe/decode_rescore_beam5_beta0.05.txt', modal='AO', nbest=1, silweight=0, type='kenlm', unitlm=False, unkweight=-inf, wordscore=2)
2023-02-05 11:16:03,374 - __main__ - INFO - 
Trained Model File: /home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_0604-wer_0.054.ckpt
2023-02-05 11:16:03,374 - __main__ - INFO - no noise
2023-02-05 11:16:17,531 - __main__ - INFO - _IncompatibleKeys(missing_keys=['transformer_lm._float_tensor', 'transformer_lm.models.0.decoder.version', 'transformer_lm.models.0.decoder.embed_tokens.weight', 'transformer_lm.models.0.decoder.embed_positions._float_tensor', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.0.fc1.weight', 'transformer_lm.models.0.decoder.layers.0.fc1.bias', 'transformer_lm.models.0.decoder.layers.0.fc2.weight', 'transformer_lm.models.0.decoder.layers.0.fc2.bias', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.fc1.weight', 'transformer_lm.models.0.decoder.layers.1.fc1.bias', 'transformer_lm.models.0.decoder.layers.1.fc2.weight', 'transformer_lm.models.0.decoder.layers.1.fc2.bias', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.fc1.weight', 'transformer_lm.models.0.decoder.layers.2.fc1.bias', 'transformer_lm.models.0.decoder.layers.2.fc2.weight', 'transformer_lm.models.0.decoder.layers.2.fc2.bias', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.fc1.weight', 'transformer_lm.models.0.decoder.layers.3.fc1.bias', 'transformer_lm.models.0.decoder.layers.3.fc2.weight', 'transformer_lm.models.0.decoder.layers.3.fc2.bias', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.fc1.weight', 'transformer_lm.models.0.decoder.layers.4.fc1.bias', 'transformer_lm.models.0.decoder.layers.4.fc2.weight', 'transformer_lm.models.0.decoder.layers.4.fc2.bias', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.fc1.weight', 'transformer_lm.models.0.decoder.layers.5.fc1.bias', 'transformer_lm.models.0.decoder.layers.5.fc2.weight', 'transformer_lm.models.0.decoder.layers.5.fc2.bias', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'transformer_lm.models.0.decoder.output_projection.weight', 'lstm_lm._float_tensor', 'lstm_lm.models.0.decoder.version', 'lstm_lm.models.0.decoder.embed_tokens.weight', 'lstm_lm.models.0.decoder.embed_positions._float_tensor', 'lstm_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'lstm_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'lstm_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'lstm_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'lstm_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'lstm_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'lstm_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'lstm_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'lstm_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.0.fc1.weight', 'lstm_lm.models.0.decoder.layers.0.fc1.bias', 'lstm_lm.models.0.decoder.layers.0.fc2.weight', 'lstm_lm.models.0.decoder.layers.0.fc2.bias', 'lstm_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'lstm_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'lstm_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'lstm_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'lstm_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'lstm_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'lstm_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'lstm_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'lstm_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.1.fc1.weight', 'lstm_lm.models.0.decoder.layers.1.fc1.bias', 'lstm_lm.models.0.decoder.layers.1.fc2.weight', 'lstm_lm.models.0.decoder.layers.1.fc2.bias', 'lstm_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'lstm_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'lstm_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'lstm_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'lstm_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'lstm_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'lstm_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'lstm_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'lstm_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.2.fc1.weight', 'lstm_lm.models.0.decoder.layers.2.fc1.bias', 'lstm_lm.models.0.decoder.layers.2.fc2.weight', 'lstm_lm.models.0.decoder.layers.2.fc2.bias', 'lstm_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'lstm_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'lstm_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'lstm_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'lstm_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'lstm_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'lstm_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'lstm_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'lstm_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.3.fc1.weight', 'lstm_lm.models.0.decoder.layers.3.fc1.bias', 'lstm_lm.models.0.decoder.layers.3.fc2.weight', 'lstm_lm.models.0.decoder.layers.3.fc2.bias', 'lstm_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'lstm_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'lstm_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'lstm_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'lstm_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'lstm_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'lstm_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'lstm_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'lstm_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.4.fc1.weight', 'lstm_lm.models.0.decoder.layers.4.fc1.bias', 'lstm_lm.models.0.decoder.layers.4.fc2.weight', 'lstm_lm.models.0.decoder.layers.4.fc2.bias', 'lstm_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'lstm_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'lstm_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'lstm_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'lstm_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'lstm_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'lstm_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'lstm_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'lstm_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'lstm_lm.models.0.decoder.layers.5.fc1.weight', 'lstm_lm.models.0.decoder.layers.5.fc1.bias', 'lstm_lm.models.0.decoder.layers.5.fc2.weight', 'lstm_lm.models.0.decoder.layers.5.fc2.bias', 'lstm_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'lstm_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'lstm_lm.models.0.decoder.output_projection.weight'], unexpected_keys=[])
2023-02-05 11:16:17,732 - __main__ - INFO - 
Testing the trained model .... 

2023-02-05 11:19:07,984 - __main__ - INFO - 

2023-02-05 11:19:07,984 - __main__ - INFO - evalWER:12,evalCCount:484
2023-02-05 11:19:07,985 - __main__ - INFO - batch1 || Test CER: 0.01312 || Test WER: 0.02479
2023-02-05 11:21:05,809 - __main__ - INFO - rescore index:1
2023-02-05 11:21:54,991 - __main__ - INFO - 

2023-02-05 11:21:54,991 - __main__ - INFO - evalWER:19,evalCCount:834
2023-02-05 11:21:54,991 - __main__ - INFO - batch2 || Test CER: 0.01283 || Test WER: 0.02278
2023-02-05 11:24:40,122 - __main__ - INFO - 

2023-02-05 11:24:40,122 - __main__ - INFO - evalWER:28,evalCCount:1188
2023-02-05 11:24:40,123 - __main__ - INFO - batch3 || Test CER: 0.01501 || Test WER: 0.02357
2023-02-05 11:27:25,093 - __main__ - INFO - 

2023-02-05 11:27:25,093 - __main__ - INFO - evalWER:32,evalCCount:1518
2023-02-05 11:27:25,093 - __main__ - INFO - batch4 || Test CER: 0.01325 || Test WER: 0.02108
2023-02-05 11:30:13,246 - __main__ - INFO - 

2023-02-05 11:30:13,246 - __main__ - INFO - evalWER:37,evalCCount:1908
2023-02-05 11:30:13,246 - __main__ - INFO - batch5 || Test CER: 0.01165 || Test WER: 0.01939
2023-02-05 11:33:02,794 - __main__ - INFO - 

2023-02-05 11:33:02,794 - __main__ - INFO - evalWER:40,evalCCount:2310
2023-02-05 11:33:02,794 - __main__ - INFO - batch6 || Test CER: 0.01037 || Test WER: 0.01732
2023-02-05 11:35:52,049 - __main__ - INFO - 

2023-02-05 11:35:52,049 - __main__ - INFO - evalWER:47,evalCCount:2719
2023-02-05 11:35:52,049 - __main__ - INFO - batch7 || Test CER: 0.01037 || Test WER: 0.01729
2023-02-05 11:38:36,326 - __main__ - INFO - 

2023-02-05 11:38:36,326 - __main__ - INFO - evalWER:52,evalCCount:3029
2023-02-05 11:38:36,327 - __main__ - INFO - batch8 || Test CER: 0.01026 || Test WER: 0.01717
2023-02-05 11:41:08,055 - __main__ - INFO - rescore index:1
2023-02-05 11:41:21,287 - __main__ - INFO - 

2023-02-05 11:41:21,287 - __main__ - INFO - evalWER:56,evalCCount:3390
2023-02-05 11:41:21,287 - __main__ - INFO - batch9 || Test CER: 0.01009 || Test WER: 0.01652
2023-02-05 11:43:58,251 - __main__ - INFO - 

2023-02-05 11:43:58,251 - __main__ - INFO - evalWER:63,evalCCount:3695
2023-02-05 11:43:58,251 - __main__ - INFO - batch10 || Test CER: 0.01008 || Test WER: 0.01705
2023-02-05 11:46:44,782 - __main__ - INFO - 

2023-02-05 11:46:44,783 - __main__ - INFO - evalWER:70,evalCCount:3997
2023-02-05 11:46:44,783 - __main__ - INFO - batch11 || Test CER: 0.01008 || Test WER: 0.01751
2023-02-05 11:49:18,490 - __main__ - INFO - 

2023-02-05 11:49:18,491 - __main__ - INFO - evalWER:72,evalCCount:4307
2023-02-05 11:49:18,491 - __main__ - INFO - batch12 || Test CER: 0.00946 || Test WER: 0.01672
2023-02-05 11:52:03,789 - __main__ - INFO - 

2023-02-05 11:52:03,790 - __main__ - INFO - evalWER:75,evalCCount:4652
2023-02-05 11:52:03,790 - __main__ - INFO - batch13 || Test CER: 0.00932 || Test WER: 0.01612
2023-02-05 11:54:44,671 - __main__ - INFO - 

2023-02-05 11:54:44,672 - __main__ - INFO - evalWER:80,evalCCount:5049
2023-02-05 11:54:44,672 - __main__ - INFO - batch14 || Test CER: 0.00903 || Test WER: 0.01584
2023-02-05 11:56:41,083 - __main__ - INFO - rescore index:1
2023-02-05 11:57:30,406 - __main__ - INFO - 

2023-02-05 11:57:30,406 - __main__ - INFO - evalWER:84,evalCCount:5379
2023-02-05 11:57:30,406 - __main__ - INFO - batch15 || Test CER: 0.00886 || Test WER: 0.01562
2023-02-05 11:59:55,813 - __main__ - INFO - 

2023-02-05 11:59:55,813 - __main__ - INFO - evalWER:89,evalCCount:5719
2023-02-05 11:59:55,813 - __main__ - INFO - batch16 || Test CER: 0.00866 || Test WER: 0.01556
2023-02-05 12:01:49,455 - __main__ - INFO - rescore index:1
2023-02-05 12:02:37,491 - __main__ - INFO - 

2023-02-05 12:02:37,491 - __main__ - INFO - evalWER:96,evalCCount:6019
2023-02-05 12:02:37,491 - __main__ - INFO - batch17 || Test CER: 0.00844 || Test WER: 0.01595
2023-02-05 12:05:24,075 - __main__ - INFO - 

2023-02-05 12:05:24,075 - __main__ - INFO - evalWER:105,evalCCount:6394
2023-02-05 12:05:24,075 - __main__ - INFO - batch18 || Test CER: 0.00873 || Test WER: 0.01642
2023-02-05 12:08:09,634 - __main__ - INFO - 

2023-02-05 12:08:09,635 - __main__ - INFO - evalWER:114,evalCCount:6716
2023-02-05 12:08:09,635 - __main__ - INFO - batch19 || Test CER: 0.00887 || Test WER: 0.01697
2023-02-05 12:10:54,961 - __main__ - INFO - 

2023-02-05 12:10:54,962 - __main__ - INFO - evalWER:121,evalCCount:7103
2023-02-05 12:10:54,962 - __main__ - INFO - batch20 || Test CER: 0.00879 || Test WER: 0.01704
2023-02-05 12:12:57,748 - __main__ - INFO - rescore index:1
2023-02-05 12:13:40,238 - __main__ - INFO - 

2023-02-05 12:13:40,238 - __main__ - INFO - evalWER:124,evalCCount:7487
2023-02-05 12:13:40,238 - __main__ - INFO - batch21 || Test CER: 0.00849 || Test WER: 0.01656
2023-02-05 12:16:29,018 - __main__ - INFO - 

2023-02-05 12:16:29,019 - __main__ - INFO - evalWER:130,evalCCount:7876
2023-02-05 12:16:29,019 - __main__ - INFO - batch22 || Test CER: 0.00842 || Test WER: 0.01651
2023-02-05 12:19:13,786 - __main__ - INFO - 

2023-02-05 12:19:13,787 - __main__ - INFO - evalWER:137,evalCCount:8243
2023-02-05 12:19:13,787 - __main__ - INFO - batch23 || Test CER: 0.00844 || Test WER: 0.01662
2023-02-05 12:21:52,981 - __main__ - INFO - 

2023-02-05 12:21:52,982 - __main__ - INFO - evalWER:141,evalCCount:8613
2023-02-05 12:21:52,982 - __main__ - INFO - batch24 || Test CER: 0.00839 || Test WER: 0.01637
2023-02-05 12:24:33,982 - __main__ - INFO - 

2023-02-05 12:24:33,983 - __main__ - INFO - evalWER:145,evalCCount:8967
2023-02-05 12:24:33,983 - __main__ - INFO - batch25 || Test CER: 0.00826 || Test WER: 0.01617
2023-02-05 12:27:16,459 - __main__ - INFO - 

2023-02-05 12:27:16,460 - __main__ - INFO - evalWER:150,evalCCount:9335
2023-02-05 12:27:16,460 - __main__ - INFO - batch26 || Test CER: 0.00815 || Test WER: 0.01607
2023-02-05 12:29:59,107 - __main__ - INFO - 

2023-02-05 12:29:59,108 - __main__ - INFO - evalWER:153,evalCCount:9689
2023-02-05 12:29:59,108 - __main__ - INFO - batch27 || Test CER: 0.00809 || Test WER: 0.01579
2023-02-05 12:31:28,757 - __main__ - INFO - 

2023-02-05 12:31:28,758 - __main__ - INFO - evalWER:163,evalCCount:9890
2023-02-05 12:31:28,758 - __main__ - INFO - batch28 || Test CER: 0.00892 || Test WER: 0.01648
2023-02-05 12:31:28,758 - __main__ - INFO - evalWER:163,evalCCount:9890
2023-02-05 12:31:28,759 - __main__ - INFO - AOMODAL || Test CER: 0.00892 || Test WER: 0.01648
2023-02-05 12:31:28,759 - __main__ - INFO - 
Testing Done.

