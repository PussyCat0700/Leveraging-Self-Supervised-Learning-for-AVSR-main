2023-02-23 07:02:14,263 - __main__ - INFO - Namespace(batch_size=48, beam=500, beamWidth=5, beamsizetoken=None, beamthreshold=100.0, beta=0.02, decode_type='HYBRID_SHALLOW_FUSION', eval_lrs3_model_file='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_0604-wer_0.054.ckpt', lexicon='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/lst/LRS23.lst', lmpath='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/LRS23_4gram.bin', lmweight=1, logname='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main_noneed/final_check_hybrid_shallow.txt', modal='AO', nbest=30, silweight=0, type='kenlm', unitlm=False, unkweight=-inf, wordscore=2)
2023-02-23 07:02:14,263 - __main__ - INFO - 
Trained Model File: /home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_0604-wer_0.054.ckpt
2023-02-23 07:02:14,263 - __main__ - INFO - no noise
2023-02-23 07:02:28,806 - __main__ - INFO - _IncompatibleKeys(missing_keys=['transformer_lm._float_tensor', 'transformer_lm.models.0.decoder.version', 'transformer_lm.models.0.decoder.embed_tokens.weight', 'transformer_lm.models.0.decoder.project_in_dim.weight', 'transformer_lm.models.0.decoder.embed_positions._float_tensor', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.0.fc1.weight', 'transformer_lm.models.0.decoder.layers.0.fc1.bias', 'transformer_lm.models.0.decoder.layers.0.fc2.weight', 'transformer_lm.models.0.decoder.layers.0.fc2.bias', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.fc1.weight', 'transformer_lm.models.0.decoder.layers.1.fc1.bias', 'transformer_lm.models.0.decoder.layers.1.fc2.weight', 'transformer_lm.models.0.decoder.layers.1.fc2.bias', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.fc1.weight', 'transformer_lm.models.0.decoder.layers.2.fc1.bias', 'transformer_lm.models.0.decoder.layers.2.fc2.weight', 'transformer_lm.models.0.decoder.layers.2.fc2.bias', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.fc1.weight', 'transformer_lm.models.0.decoder.layers.3.fc1.bias', 'transformer_lm.models.0.decoder.layers.3.fc2.weight', 'transformer_lm.models.0.decoder.layers.3.fc2.bias', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.fc1.weight', 'transformer_lm.models.0.decoder.layers.4.fc1.bias', 'transformer_lm.models.0.decoder.layers.4.fc2.weight', 'transformer_lm.models.0.decoder.layers.4.fc2.bias', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.fc1.weight', 'transformer_lm.models.0.decoder.layers.5.fc1.bias', 'transformer_lm.models.0.decoder.layers.5.fc2.weight', 'transformer_lm.models.0.decoder.layers.5.fc2.bias', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.fc1.weight', 'transformer_lm.models.0.decoder.layers.6.fc1.bias', 'transformer_lm.models.0.decoder.layers.6.fc2.weight', 'transformer_lm.models.0.decoder.layers.6.fc2.bias', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.fc1.weight', 'transformer_lm.models.0.decoder.layers.7.fc1.bias', 'transformer_lm.models.0.decoder.layers.7.fc2.weight', 'transformer_lm.models.0.decoder.layers.7.fc2.bias', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.fc1.weight', 'transformer_lm.models.0.decoder.layers.8.fc1.bias', 'transformer_lm.models.0.decoder.layers.8.fc2.weight', 'transformer_lm.models.0.decoder.layers.8.fc2.bias', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.fc1.weight', 'transformer_lm.models.0.decoder.layers.9.fc1.bias', 'transformer_lm.models.0.decoder.layers.9.fc2.weight', 'transformer_lm.models.0.decoder.layers.9.fc2.bias', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.fc1.weight', 'transformer_lm.models.0.decoder.layers.10.fc1.bias', 'transformer_lm.models.0.decoder.layers.10.fc2.weight', 'transformer_lm.models.0.decoder.layers.10.fc2.bias', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.fc1.weight', 'transformer_lm.models.0.decoder.layers.11.fc1.bias', 'transformer_lm.models.0.decoder.layers.11.fc2.weight', 'transformer_lm.models.0.decoder.layers.11.fc2.bias', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.bias', 'transformer_lm.models.0.decoder.project_out_dim.weight', 'transformer_lm.models.0.decoder.output_projection.weight'], unexpected_keys=[])
2023-02-23 07:02:29,039 - __main__ - INFO - 
Testing the trained model .... 

2023-02-23 07:04:01,712 - __main__ - INFO - 

2023-02-23 07:04:01,712 - __main__ - INFO - evalWER:13,evalWCount:484
2023-02-23 07:04:01,712 - __main__ - INFO - batch1 || Test CER: 0.01394 || Test WER: 0.02686
2023-02-23 07:05:31,123 - __main__ - INFO - 

2023-02-23 07:05:31,124 - __main__ - INFO - evalWER:21,evalWCount:834
2023-02-23 07:05:31,124 - __main__ - INFO - batch2 || Test CER: 0.01402 || Test WER: 0.02518
2023-02-23 07:06:59,202 - __main__ - INFO - 

2023-02-23 07:06:59,203 - __main__ - INFO - evalWER:30,evalWCount:1188
2023-02-23 07:06:59,203 - __main__ - INFO - batch3 || Test CER: 0.01584 || Test WER: 0.02525
2023-02-23 07:08:26,061 - __main__ - INFO - 

2023-02-23 07:08:26,062 - __main__ - INFO - evalWER:34,evalWCount:1518
2023-02-23 07:08:26,062 - __main__ - INFO - batch4 || Test CER: 0.01390 || Test WER: 0.02240
2023-02-23 07:09:55,321 - __main__ - INFO - 

2023-02-23 07:09:55,321 - __main__ - INFO - evalWER:39,evalWCount:1908
2023-02-23 07:09:55,321 - __main__ - INFO - batch5 || Test CER: 0.01207 || Test WER: 0.02044
2023-02-23 07:11:26,680 - __main__ - INFO - 

2023-02-23 07:11:26,680 - __main__ - INFO - evalWER:42,evalWCount:2310
2023-02-23 07:11:26,680 - __main__ - INFO - batch6 || Test CER: 0.01072 || Test WER: 0.01818
2023-02-23 07:12:58,714 - __main__ - INFO - 

2023-02-23 07:12:58,715 - __main__ - INFO - evalWER:49,evalWCount:2719
2023-02-23 07:12:58,715 - __main__ - INFO - batch7 || Test CER: 0.01066 || Test WER: 0.01802
2023-02-23 07:14:25,370 - __main__ - INFO - 

2023-02-23 07:14:25,371 - __main__ - INFO - evalWER:54,evalWCount:3029
2023-02-23 07:14:25,371 - __main__ - INFO - batch8 || Test CER: 0.01053 || Test WER: 0.01783
2023-02-23 07:15:54,485 - __main__ - INFO - 

2023-02-23 07:15:54,485 - __main__ - INFO - evalWER:58,evalWCount:3390
2023-02-23 07:15:54,485 - __main__ - INFO - batch9 || Test CER: 0.01050 || Test WER: 0.01711
2023-02-23 07:17:15,315 - __main__ - INFO - 

2023-02-23 07:17:15,315 - __main__ - INFO - evalWER:65,evalWCount:3695
2023-02-23 07:17:15,315 - __main__ - INFO - batch10 || Test CER: 0.01046 || Test WER: 0.01759
2023-02-23 07:18:45,293 - __main__ - INFO - 

2023-02-23 07:18:45,293 - __main__ - INFO - evalWER:72,evalWCount:3997
2023-02-23 07:18:45,294 - __main__ - INFO - batch11 || Test CER: 0.01043 || Test WER: 0.01801
2023-02-23 07:20:03,059 - __main__ - INFO - 

2023-02-23 07:20:03,059 - __main__ - INFO - evalWER:74,evalWCount:4307
2023-02-23 07:20:03,059 - __main__ - INFO - batch12 || Test CER: 0.00978 || Test WER: 0.01718
2023-02-23 07:21:35,629 - __main__ - INFO - 

2023-02-23 07:21:35,629 - __main__ - INFO - evalWER:77,evalWCount:4652
2023-02-23 07:21:35,629 - __main__ - INFO - batch13 || Test CER: 0.00962 || Test WER: 0.01655
2023-02-23 07:22:59,161 - __main__ - INFO - 

2023-02-23 07:22:59,161 - __main__ - INFO - evalWER:82,evalWCount:5049
2023-02-23 07:22:59,161 - __main__ - INFO - batch14 || Test CER: 0.00931 || Test WER: 0.01624
2023-02-23 07:24:28,831 - __main__ - INFO - 

2023-02-23 07:24:28,831 - __main__ - INFO - evalWER:87,evalWCount:5379
2023-02-23 07:24:28,831 - __main__ - INFO - batch15 || Test CER: 0.00916 || Test WER: 0.01617
2023-02-23 07:25:35,376 - __main__ - INFO - 

2023-02-23 07:25:35,376 - __main__ - INFO - evalWER:92,evalWCount:5719
2023-02-23 07:25:35,376 - __main__ - INFO - batch16 || Test CER: 0.00894 || Test WER: 0.01609
2023-02-23 07:27:00,336 - __main__ - INFO - 

2023-02-23 07:27:00,336 - __main__ - INFO - evalWER:100,evalWCount:6019
2023-02-23 07:27:00,336 - __main__ - INFO - batch17 || Test CER: 0.00878 || Test WER: 0.01661
2023-02-23 07:28:30,914 - __main__ - INFO - 

2023-02-23 07:28:30,914 - __main__ - INFO - evalWER:109,evalWCount:6394
2023-02-23 07:28:30,914 - __main__ - INFO - batch18 || Test CER: 0.00904 || Test WER: 0.01705
2023-02-23 07:29:59,179 - __main__ - INFO - 

2023-02-23 07:29:59,179 - __main__ - INFO - evalWER:118,evalWCount:6716
2023-02-23 07:29:59,179 - __main__ - INFO - batch19 || Test CER: 0.00917 || Test WER: 0.01757
2023-02-23 07:31:27,930 - __main__ - INFO - 

2023-02-23 07:31:27,930 - __main__ - INFO - evalWER:125,evalWCount:7103
2023-02-23 07:31:27,930 - __main__ - INFO - batch20 || Test CER: 0.00907 || Test WER: 0.01760
2023-02-23 07:32:54,778 - __main__ - INFO - 

2023-02-23 07:32:54,778 - __main__ - INFO - evalWER:130,evalWCount:7487
2023-02-23 07:32:54,778 - __main__ - INFO - batch21 || Test CER: 0.00878 || Test WER: 0.01736
2023-02-23 07:34:27,049 - __main__ - INFO - 

2023-02-23 07:34:27,049 - __main__ - INFO - evalWER:136,evalWCount:7876
2023-02-23 07:34:27,049 - __main__ - INFO - batch22 || Test CER: 0.00870 || Test WER: 0.01727
2023-02-23 07:35:57,780 - __main__ - INFO - 

2023-02-23 07:35:57,781 - __main__ - INFO - evalWER:143,evalWCount:8243
2023-02-23 07:35:57,781 - __main__ - INFO - batch23 || Test CER: 0.00870 || Test WER: 0.01735
2023-02-23 07:37:19,729 - __main__ - INFO - 

2023-02-23 07:37:19,729 - __main__ - INFO - evalWER:147,evalWCount:8613
2023-02-23 07:37:19,729 - __main__ - INFO - batch24 || Test CER: 0.00865 || Test WER: 0.01707
2023-02-23 07:38:43,063 - __main__ - INFO - 

2023-02-23 07:38:43,063 - __main__ - INFO - evalWER:151,evalWCount:8967
2023-02-23 07:38:43,063 - __main__ - INFO - batch25 || Test CER: 0.00850 || Test WER: 0.01684
2023-02-23 07:40:09,301 - __main__ - INFO - 

2023-02-23 07:40:09,301 - __main__ - INFO - evalWER:156,evalWCount:9335
2023-02-23 07:40:09,301 - __main__ - INFO - batch26 || Test CER: 0.00839 || Test WER: 0.01671
2023-02-23 07:41:35,919 - __main__ - INFO - 

2023-02-23 07:41:35,919 - __main__ - INFO - evalWER:159,evalWCount:9689
2023-02-23 07:41:35,920 - __main__ - INFO - batch27 || Test CER: 0.00832 || Test WER: 0.01641
2023-02-23 07:42:34,380 - __main__ - INFO - 

2023-02-23 07:42:34,380 - __main__ - INFO - evalWER:169,evalWCount:9890
2023-02-23 07:42:34,380 - __main__ - INFO - batch28 || Test CER: 0.00915 || Test WER: 0.01709
2023-02-23 07:42:34,381 - __main__ - INFO - evalWER:169,evalCCount:9890
2023-02-23 07:42:34,381 - __main__ - INFO - AOMODAL || Test CER: 0.00915 || Test WER: 0.01709
2023-02-23 07:42:34,381 - __main__ - INFO - 
Testing Done.

