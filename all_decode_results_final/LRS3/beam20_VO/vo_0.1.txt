2023-03-01 13:24:58,822 - __main__ - INFO - Namespace(batch_size=64, beam=500, beamWidth=20, beamsizetoken=None, beamthreshold=100.0, beta=0.1, decode_type='HYBRID_RESCORE', eval_lrs3_model_file='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt', lexicon='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/lst/LRS23.lst', lmpath='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/LRS23_4gram.bin', lmweight=1, logname='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main_noneed/beam20/vo_0.1.txt', modal='VO', nbest=30, silweight=0, type='kenlm', unitlm=False, unkweight=-inf, wordscore=2)
2023-03-01 13:24:58,822 - __main__ - INFO - 
Trained Model File: /home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt
2023-03-01 13:24:58,822 - __main__ - INFO - no noise
2023-03-01 13:25:03,298 - __main__ - INFO - _IncompatibleKeys(missing_keys=['transformer_lm._float_tensor', 'transformer_lm.models.0.decoder.version', 'transformer_lm.models.0.decoder.embed_tokens.weight', 'transformer_lm.models.0.decoder.project_in_dim.weight', 'transformer_lm.models.0.decoder.embed_positions._float_tensor', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.0.fc1.weight', 'transformer_lm.models.0.decoder.layers.0.fc1.bias', 'transformer_lm.models.0.decoder.layers.0.fc2.weight', 'transformer_lm.models.0.decoder.layers.0.fc2.bias', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.fc1.weight', 'transformer_lm.models.0.decoder.layers.1.fc1.bias', 'transformer_lm.models.0.decoder.layers.1.fc2.weight', 'transformer_lm.models.0.decoder.layers.1.fc2.bias', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.fc1.weight', 'transformer_lm.models.0.decoder.layers.2.fc1.bias', 'transformer_lm.models.0.decoder.layers.2.fc2.weight', 'transformer_lm.models.0.decoder.layers.2.fc2.bias', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.fc1.weight', 'transformer_lm.models.0.decoder.layers.3.fc1.bias', 'transformer_lm.models.0.decoder.layers.3.fc2.weight', 'transformer_lm.models.0.decoder.layers.3.fc2.bias', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.fc1.weight', 'transformer_lm.models.0.decoder.layers.4.fc1.bias', 'transformer_lm.models.0.decoder.layers.4.fc2.weight', 'transformer_lm.models.0.decoder.layers.4.fc2.bias', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.fc1.weight', 'transformer_lm.models.0.decoder.layers.5.fc1.bias', 'transformer_lm.models.0.decoder.layers.5.fc2.weight', 'transformer_lm.models.0.decoder.layers.5.fc2.bias', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.fc1.weight', 'transformer_lm.models.0.decoder.layers.6.fc1.bias', 'transformer_lm.models.0.decoder.layers.6.fc2.weight', 'transformer_lm.models.0.decoder.layers.6.fc2.bias', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.fc1.weight', 'transformer_lm.models.0.decoder.layers.7.fc1.bias', 'transformer_lm.models.0.decoder.layers.7.fc2.weight', 'transformer_lm.models.0.decoder.layers.7.fc2.bias', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.fc1.weight', 'transformer_lm.models.0.decoder.layers.8.fc1.bias', 'transformer_lm.models.0.decoder.layers.8.fc2.weight', 'transformer_lm.models.0.decoder.layers.8.fc2.bias', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.fc1.weight', 'transformer_lm.models.0.decoder.layers.9.fc1.bias', 'transformer_lm.models.0.decoder.layers.9.fc2.weight', 'transformer_lm.models.0.decoder.layers.9.fc2.bias', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.fc1.weight', 'transformer_lm.models.0.decoder.layers.10.fc1.bias', 'transformer_lm.models.0.decoder.layers.10.fc2.weight', 'transformer_lm.models.0.decoder.layers.10.fc2.bias', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.fc1.weight', 'transformer_lm.models.0.decoder.layers.11.fc1.bias', 'transformer_lm.models.0.decoder.layers.11.fc2.weight', 'transformer_lm.models.0.decoder.layers.11.fc2.bias', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.bias', 'transformer_lm.models.0.decoder.project_out_dim.weight', 'transformer_lm.models.0.decoder.output_projection.weight'], unexpected_keys=[])
2023-03-01 13:25:03,375 - __main__ - INFO - 
Testing the trained model .... 

2023-03-01 13:32:12,183 - __main__ - INFO - rescore index:1
2023-03-01 13:32:15,364 - __main__ - INFO - rescore index:5
2023-03-01 13:32:20,465 - __main__ - INFO - rescore index:1
2023-03-01 13:32:28,552 - __main__ - INFO - rescore index:2
2023-03-01 13:32:29,190 - __main__ - INFO - rescore index:5
2023-03-01 13:32:33,623 - __main__ - INFO - rescore index:6
2023-03-01 13:32:43,562 - __main__ - INFO - rescore index:1
2023-03-01 13:32:45,450 - __main__ - INFO - rescore index:4
2023-03-01 13:32:46,094 - __main__ - INFO - rescore index:1
2023-03-01 13:32:48,644 - __main__ - INFO - rescore index:1
2023-03-01 13:32:49,295 - __main__ - INFO - rescore index:2
2023-03-01 13:32:51,202 - __main__ - INFO - rescore index:1
2023-03-01 13:32:52,459 - __main__ - INFO - rescore index:1
2023-03-01 13:32:52,968 - __main__ - INFO - 

2023-03-01 13:32:52,968 - __main__ - INFO - evalWER:219,evalWCount:625
2023-03-01 13:32:52,968 - __main__ - INFO - batch1 || Test CER: 0.23445 || Test WER: 0.35040
2023-03-01 13:39:41,149 - __main__ - INFO - rescore index:1
2023-03-01 13:39:44,935 - __main__ - INFO - rescore index:2
2023-03-01 13:39:46,839 - __main__ - INFO - rescore index:1
2023-03-01 13:39:49,185 - __main__ - INFO - rescore index:6
2023-03-01 13:39:52,950 - __main__ - INFO - rescore index:1
2023-03-01 13:39:54,834 - __main__ - INFO - rescore index:1
2023-03-01 13:40:03,483 - __main__ - INFO - rescore index:2
2023-03-01 13:40:08,524 - __main__ - INFO - rescore index:1
2023-03-01 13:40:10,421 - __main__ - INFO - rescore index:4
2023-03-01 13:40:11,053 - __main__ - INFO - rescore index:1
2023-03-01 13:40:11,680 - __main__ - INFO - rescore index:1
2023-03-01 13:40:18,931 - __main__ - INFO - 

2023-03-01 13:40:18,931 - __main__ - INFO - evalWER:438,evalWCount:1084
2023-03-01 13:40:18,931 - __main__ - INFO - batch2 || Test CER: 0.27155 || Test WER: 0.40406
2023-03-01 13:47:04,816 - __main__ - INFO - rescore index:1
2023-03-01 13:47:06,697 - __main__ - INFO - rescore index:9
2023-03-01 13:47:09,221 - __main__ - INFO - rescore index:3
2023-03-01 13:47:10,482 - __main__ - INFO - rescore index:2
2023-03-01 13:47:11,118 - __main__ - INFO - rescore index:2
2023-03-01 13:47:15,334 - __main__ - INFO - rescore index:1
2023-03-01 13:47:16,585 - __main__ - INFO - rescore index:1
2023-03-01 13:47:22,861 - __main__ - INFO - rescore index:2
2023-03-01 13:47:24,764 - __main__ - INFO - rescore index:1
2023-03-01 13:47:27,293 - __main__ - INFO - rescore index:3
2023-03-01 13:47:29,629 - __main__ - INFO - rescore index:1
2023-03-01 13:47:30,254 - __main__ - INFO - rescore index:1
2023-03-01 13:47:35,898 - __main__ - INFO - rescore index:3
2023-03-01 13:47:40,943 - __main__ - INFO - rescore index:2
2023-03-01 13:47:43,780 - __main__ - INFO - 

2023-03-01 13:47:43,781 - __main__ - INFO - evalWER:681,evalWCount:1518
2023-03-01 13:47:43,781 - __main__ - INFO - batch3 || Test CER: 0.30155 || Test WER: 0.44862
2023-03-01 13:54:44,488 - __main__ - INFO - rescore index:1
2023-03-01 13:54:45,800 - __main__ - INFO - rescore index:7
2023-03-01 13:54:46,453 - __main__ - INFO - rescore index:1
2023-03-01 13:54:47,763 - __main__ - INFO - rescore index:1
2023-03-01 13:54:49,088 - __main__ - INFO - rescore index:21
2023-03-01 13:54:49,750 - __main__ - INFO - rescore index:4
2023-03-01 13:54:51,707 - __main__ - INFO - rescore index:1
2023-03-01 13:54:55,990 - __main__ - INFO - rescore index:3
2023-03-01 13:54:57,300 - __main__ - INFO - rescore index:1
2023-03-01 13:55:00,558 - __main__ - INFO - rescore index:1
2023-03-01 13:55:02,570 - __main__ - INFO - rescore index:2
2023-03-01 13:55:07,184 - __main__ - INFO - rescore index:1
2023-03-01 13:55:10,194 - __main__ - INFO - rescore index:1
2023-03-01 13:55:12,802 - __main__ - INFO - rescore index:2
2023-03-01 13:55:16,073 - __main__ - INFO - rescore index:2
2023-03-01 13:55:17,384 - __main__ - INFO - rescore index:2
2023-03-01 13:55:18,688 - __main__ - INFO - rescore index:1
2023-03-01 13:55:20,662 - __main__ - INFO - rescore index:1
2023-03-01 13:55:21,026 - __main__ - INFO - 

2023-03-01 13:55:21,026 - __main__ - INFO - evalWER:947,evalWCount:2021
2023-03-01 13:55:21,026 - __main__ - INFO - batch4 || Test CER: 0.31837 || Test WER: 0.46858
2023-03-01 14:02:22,955 - __main__ - INFO - rescore index:4
2023-03-01 14:02:34,186 - __main__ - INFO - rescore index:3
2023-03-01 14:02:34,841 - __main__ - INFO - rescore index:1
2023-03-01 14:02:36,520 - __main__ - INFO - rescore index:4
2023-03-01 14:02:37,822 - __main__ - INFO - rescore index:1
2023-03-01 14:02:40,437 - __main__ - INFO - rescore index:1
2023-03-01 14:02:45,011 - __main__ - INFO - rescore index:8
2023-03-01 14:02:46,325 - __main__ - INFO - rescore index:4
2023-03-01 14:02:48,954 - __main__ - INFO - rescore index:1
2023-03-01 14:02:51,314 - __main__ - INFO - rescore index:3
2023-03-01 14:02:55,208 - __main__ - INFO - rescore index:1
2023-03-01 14:02:57,168 - __main__ - INFO - rescore index:3
2023-03-01 14:02:59,789 - __main__ - INFO - rescore index:2
2023-03-01 14:03:03,728 - __main__ - INFO - rescore index:2
2023-03-01 14:03:04,193 - __main__ - INFO - 

2023-03-01 14:03:04,193 - __main__ - INFO - evalWER:1175,evalWCount:2599
2023-03-01 14:03:04,193 - __main__ - INFO - batch5 || Test CER: 0.30714 || Test WER: 0.45210
2023-03-01 14:10:03,001 - __main__ - INFO - rescore index:2
2023-03-01 14:10:04,290 - __main__ - INFO - rescore index:2
2023-03-01 14:10:06,247 - __main__ - INFO - rescore index:4
2023-03-01 14:10:08,189 - __main__ - INFO - rescore index:1
2023-03-01 14:10:09,501 - __main__ - INFO - rescore index:2
2023-03-01 14:10:12,466 - __main__ - INFO - rescore index:1
2023-03-01 14:10:15,690 - __main__ - INFO - rescore index:1
2023-03-01 14:10:19,597 - __main__ - INFO - rescore index:1
2023-03-01 14:10:21,553 - __main__ - INFO - rescore index:1
2023-03-01 14:10:22,213 - __main__ - INFO - rescore index:18
2023-03-01 14:10:25,490 - __main__ - INFO - rescore index:1
2023-03-01 14:10:27,838 - __main__ - INFO - rescore index:1
2023-03-01 14:10:30,437 - __main__ - INFO - rescore index:1
2023-03-01 14:10:38,909 - __main__ - INFO - rescore index:5
2023-03-01 14:10:45,631 - __main__ - INFO - 

2023-03-01 14:10:45,631 - __main__ - INFO - evalWER:1375,evalWCount:3029
2023-03-01 14:10:45,631 - __main__ - INFO - batch6 || Test CER: 0.30749 || Test WER: 0.45395
2023-03-01 14:17:41,239 - __main__ - INFO - rescore index:1
2023-03-01 14:17:44,856 - __main__ - INFO - rescore index:2
2023-03-01 14:17:46,803 - __main__ - INFO - rescore index:1
2023-03-01 14:17:47,451 - __main__ - INFO - rescore index:13
2023-03-01 14:17:49,407 - __main__ - INFO - rescore index:2
2023-03-01 14:17:56,649 - __main__ - INFO - rescore index:1
2023-03-01 14:17:58,603 - __main__ - INFO - rescore index:1
2023-03-01 14:18:00,291 - __main__ - INFO - rescore index:1
2023-03-01 14:18:00,933 - __main__ - INFO - rescore index:4
2023-03-01 14:18:04,162 - __main__ - INFO - rescore index:6
2023-03-01 14:18:07,406 - __main__ - INFO - rescore index:2
2023-03-01 14:18:10,701 - __main__ - INFO - rescore index:1
2023-03-01 14:18:15,632 - __main__ - INFO - rescore index:2
2023-03-01 14:18:18,230 - __main__ - INFO - rescore index:2
2023-03-01 14:18:20,181 - __main__ - INFO - rescore index:2
2023-03-01 14:18:22,131 - __main__ - INFO - rescore index:2
2023-03-01 14:18:23,877 - __main__ - INFO - 

2023-03-01 14:18:23,877 - __main__ - INFO - evalWER:1586,evalWCount:3496
2023-03-01 14:18:23,877 - __main__ - INFO - batch7 || Test CER: 0.30868 || Test WER: 0.45366
2023-03-01 14:45:48,162 - __main__ - INFO - Namespace(batch_size=64, beam=500, beamWidth=20, beamsizetoken=None, beamthreshold=100.0, beta=0.1, decode_type='HYBRID_RESCORE', eval_lrs3_model_file='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt', lexicon='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/lst/LRS23.lst', lmpath='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/LRS23_4gram.bin', lmweight=1, logname='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main_noneed/beam20/vo_0.1.txt', modal='VO', nbest=30, silweight=0, type='kenlm', unitlm=False, unkweight=-inf, wordscore=2)
2023-03-01 14:45:48,163 - __main__ - INFO - 
Trained Model File: /home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt
2023-03-01 14:45:48,163 - __main__ - INFO - no noise
2023-03-01 14:45:53,148 - __main__ - INFO - _IncompatibleKeys(missing_keys=['transformer_lm._float_tensor', 'transformer_lm.models.0.decoder.version', 'transformer_lm.models.0.decoder.embed_tokens.weight', 'transformer_lm.models.0.decoder.project_in_dim.weight', 'transformer_lm.models.0.decoder.embed_positions._float_tensor', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.0.fc1.weight', 'transformer_lm.models.0.decoder.layers.0.fc1.bias', 'transformer_lm.models.0.decoder.layers.0.fc2.weight', 'transformer_lm.models.0.decoder.layers.0.fc2.bias', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.fc1.weight', 'transformer_lm.models.0.decoder.layers.1.fc1.bias', 'transformer_lm.models.0.decoder.layers.1.fc2.weight', 'transformer_lm.models.0.decoder.layers.1.fc2.bias', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.fc1.weight', 'transformer_lm.models.0.decoder.layers.2.fc1.bias', 'transformer_lm.models.0.decoder.layers.2.fc2.weight', 'transformer_lm.models.0.decoder.layers.2.fc2.bias', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.fc1.weight', 'transformer_lm.models.0.decoder.layers.3.fc1.bias', 'transformer_lm.models.0.decoder.layers.3.fc2.weight', 'transformer_lm.models.0.decoder.layers.3.fc2.bias', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.fc1.weight', 'transformer_lm.models.0.decoder.layers.4.fc1.bias', 'transformer_lm.models.0.decoder.layers.4.fc2.weight', 'transformer_lm.models.0.decoder.layers.4.fc2.bias', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.fc1.weight', 'transformer_lm.models.0.decoder.layers.5.fc1.bias', 'transformer_lm.models.0.decoder.layers.5.fc2.weight', 'transformer_lm.models.0.decoder.layers.5.fc2.bias', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.fc1.weight', 'transformer_lm.models.0.decoder.layers.6.fc1.bias', 'transformer_lm.models.0.decoder.layers.6.fc2.weight', 'transformer_lm.models.0.decoder.layers.6.fc2.bias', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.fc1.weight', 'transformer_lm.models.0.decoder.layers.7.fc1.bias', 'transformer_lm.models.0.decoder.layers.7.fc2.weight', 'transformer_lm.models.0.decoder.layers.7.fc2.bias', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.fc1.weight', 'transformer_lm.models.0.decoder.layers.8.fc1.bias', 'transformer_lm.models.0.decoder.layers.8.fc2.weight', 'transformer_lm.models.0.decoder.layers.8.fc2.bias', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.fc1.weight', 'transformer_lm.models.0.decoder.layers.9.fc1.bias', 'transformer_lm.models.0.decoder.layers.9.fc2.weight', 'transformer_lm.models.0.decoder.layers.9.fc2.bias', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.fc1.weight', 'transformer_lm.models.0.decoder.layers.10.fc1.bias', 'transformer_lm.models.0.decoder.layers.10.fc2.weight', 'transformer_lm.models.0.decoder.layers.10.fc2.bias', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.fc1.weight', 'transformer_lm.models.0.decoder.layers.11.fc1.bias', 'transformer_lm.models.0.decoder.layers.11.fc2.weight', 'transformer_lm.models.0.decoder.layers.11.fc2.bias', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.bias', 'transformer_lm.models.0.decoder.project_out_dim.weight', 'transformer_lm.models.0.decoder.output_projection.weight'], unexpected_keys=[])
2023-03-01 14:45:53,293 - __main__ - INFO - 
Testing the trained model .... 

2023-03-01 14:45:53,293 - __main__ - INFO - beamWidth:20.000000
2023-03-01 14:47:11,639 - __main__ - INFO - Namespace(batch_size=64, beam=500, beamWidth=20, beamsizetoken=None, beamthreshold=100.0, beta=0.1, decode_type='HYBRID_RESCORE', eval_lrs3_model_file='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt', lexicon='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/lst/LRS23.lst', lmpath='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/LRS23_4gram.bin', lmweight=1, logname='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main_noneed/beam20/vo_0.1.txt', modal='VO', nbest=30, silweight=0, type='kenlm', unitlm=False, unkweight=-inf, wordscore=2)
2023-03-01 14:47:11,639 - __main__ - INFO - 
Trained Model File: /home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt
2023-03-01 14:47:11,639 - __main__ - INFO - no noise
2023-03-01 14:47:16,314 - __main__ - INFO - _IncompatibleKeys(missing_keys=['transformer_lm._float_tensor', 'transformer_lm.models.0.decoder.version', 'transformer_lm.models.0.decoder.embed_tokens.weight', 'transformer_lm.models.0.decoder.project_in_dim.weight', 'transformer_lm.models.0.decoder.embed_positions._float_tensor', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.0.fc1.weight', 'transformer_lm.models.0.decoder.layers.0.fc1.bias', 'transformer_lm.models.0.decoder.layers.0.fc2.weight', 'transformer_lm.models.0.decoder.layers.0.fc2.bias', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.fc1.weight', 'transformer_lm.models.0.decoder.layers.1.fc1.bias', 'transformer_lm.models.0.decoder.layers.1.fc2.weight', 'transformer_lm.models.0.decoder.layers.1.fc2.bias', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.fc1.weight', 'transformer_lm.models.0.decoder.layers.2.fc1.bias', 'transformer_lm.models.0.decoder.layers.2.fc2.weight', 'transformer_lm.models.0.decoder.layers.2.fc2.bias', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.fc1.weight', 'transformer_lm.models.0.decoder.layers.3.fc1.bias', 'transformer_lm.models.0.decoder.layers.3.fc2.weight', 'transformer_lm.models.0.decoder.layers.3.fc2.bias', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.fc1.weight', 'transformer_lm.models.0.decoder.layers.4.fc1.bias', 'transformer_lm.models.0.decoder.layers.4.fc2.weight', 'transformer_lm.models.0.decoder.layers.4.fc2.bias', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.fc1.weight', 'transformer_lm.models.0.decoder.layers.5.fc1.bias', 'transformer_lm.models.0.decoder.layers.5.fc2.weight', 'transformer_lm.models.0.decoder.layers.5.fc2.bias', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.fc1.weight', 'transformer_lm.models.0.decoder.layers.6.fc1.bias', 'transformer_lm.models.0.decoder.layers.6.fc2.weight', 'transformer_lm.models.0.decoder.layers.6.fc2.bias', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.fc1.weight', 'transformer_lm.models.0.decoder.layers.7.fc1.bias', 'transformer_lm.models.0.decoder.layers.7.fc2.weight', 'transformer_lm.models.0.decoder.layers.7.fc2.bias', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.fc1.weight', 'transformer_lm.models.0.decoder.layers.8.fc1.bias', 'transformer_lm.models.0.decoder.layers.8.fc2.weight', 'transformer_lm.models.0.decoder.layers.8.fc2.bias', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.fc1.weight', 'transformer_lm.models.0.decoder.layers.9.fc1.bias', 'transformer_lm.models.0.decoder.layers.9.fc2.weight', 'transformer_lm.models.0.decoder.layers.9.fc2.bias', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.fc1.weight', 'transformer_lm.models.0.decoder.layers.10.fc1.bias', 'transformer_lm.models.0.decoder.layers.10.fc2.weight', 'transformer_lm.models.0.decoder.layers.10.fc2.bias', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.fc1.weight', 'transformer_lm.models.0.decoder.layers.11.fc1.bias', 'transformer_lm.models.0.decoder.layers.11.fc2.weight', 'transformer_lm.models.0.decoder.layers.11.fc2.bias', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.bias', 'transformer_lm.models.0.decoder.project_out_dim.weight', 'transformer_lm.models.0.decoder.output_projection.weight'], unexpected_keys=[])
2023-03-01 14:47:16,441 - __main__ - INFO - 
Testing the trained model .... 

2023-03-01 14:47:16,441 - __main__ - INFO - beamWidth:20.000000
2023-03-01 14:50:15,911 - __main__ - INFO - Namespace(batch_size=64, beam=500, beamWidth=20, beamsizetoken=None, beamthreshold=100.0, beta=0.1, decode_type='HYBRID_RESCORE', eval_lrs3_model_file='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt', lexicon='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/lst/LRS23.lst', lmpath='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/LRS23_4gram.bin', lmweight=1, logname='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main_noneed/beam20/vo_0.1.txt', modal='VO', nbest=30, silweight=0, type='kenlm', unitlm=False, unkweight=-inf, wordscore=2)
2023-03-01 14:50:15,912 - __main__ - INFO - 
Trained Model File: /home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt
2023-03-01 14:50:15,912 - __main__ - INFO - no noise
2023-03-01 14:50:20,422 - __main__ - INFO - _IncompatibleKeys(missing_keys=['transformer_lm._float_tensor', 'transformer_lm.models.0.decoder.version', 'transformer_lm.models.0.decoder.embed_tokens.weight', 'transformer_lm.models.0.decoder.project_in_dim.weight', 'transformer_lm.models.0.decoder.embed_positions._float_tensor', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.0.fc1.weight', 'transformer_lm.models.0.decoder.layers.0.fc1.bias', 'transformer_lm.models.0.decoder.layers.0.fc2.weight', 'transformer_lm.models.0.decoder.layers.0.fc2.bias', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.fc1.weight', 'transformer_lm.models.0.decoder.layers.1.fc1.bias', 'transformer_lm.models.0.decoder.layers.1.fc2.weight', 'transformer_lm.models.0.decoder.layers.1.fc2.bias', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.fc1.weight', 'transformer_lm.models.0.decoder.layers.2.fc1.bias', 'transformer_lm.models.0.decoder.layers.2.fc2.weight', 'transformer_lm.models.0.decoder.layers.2.fc2.bias', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.fc1.weight', 'transformer_lm.models.0.decoder.layers.3.fc1.bias', 'transformer_lm.models.0.decoder.layers.3.fc2.weight', 'transformer_lm.models.0.decoder.layers.3.fc2.bias', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.fc1.weight', 'transformer_lm.models.0.decoder.layers.4.fc1.bias', 'transformer_lm.models.0.decoder.layers.4.fc2.weight', 'transformer_lm.models.0.decoder.layers.4.fc2.bias', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.fc1.weight', 'transformer_lm.models.0.decoder.layers.5.fc1.bias', 'transformer_lm.models.0.decoder.layers.5.fc2.weight', 'transformer_lm.models.0.decoder.layers.5.fc2.bias', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.fc1.weight', 'transformer_lm.models.0.decoder.layers.6.fc1.bias', 'transformer_lm.models.0.decoder.layers.6.fc2.weight', 'transformer_lm.models.0.decoder.layers.6.fc2.bias', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.fc1.weight', 'transformer_lm.models.0.decoder.layers.7.fc1.bias', 'transformer_lm.models.0.decoder.layers.7.fc2.weight', 'transformer_lm.models.0.decoder.layers.7.fc2.bias', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.fc1.weight', 'transformer_lm.models.0.decoder.layers.8.fc1.bias', 'transformer_lm.models.0.decoder.layers.8.fc2.weight', 'transformer_lm.models.0.decoder.layers.8.fc2.bias', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.fc1.weight', 'transformer_lm.models.0.decoder.layers.9.fc1.bias', 'transformer_lm.models.0.decoder.layers.9.fc2.weight', 'transformer_lm.models.0.decoder.layers.9.fc2.bias', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.fc1.weight', 'transformer_lm.models.0.decoder.layers.10.fc1.bias', 'transformer_lm.models.0.decoder.layers.10.fc2.weight', 'transformer_lm.models.0.decoder.layers.10.fc2.bias', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.fc1.weight', 'transformer_lm.models.0.decoder.layers.11.fc1.bias', 'transformer_lm.models.0.decoder.layers.11.fc2.weight', 'transformer_lm.models.0.decoder.layers.11.fc2.bias', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.bias', 'transformer_lm.models.0.decoder.project_out_dim.weight', 'transformer_lm.models.0.decoder.output_projection.weight'], unexpected_keys=[])
2023-03-01 14:50:20,548 - __main__ - INFO - 
Testing the trained model .... 

2023-03-01 14:50:20,549 - __main__ - INFO - beamWidth:20.000000
2023-03-01 14:53:02,989 - __main__ - INFO - rescore index:1
2023-03-01 14:53:06,244 - __main__ - INFO - rescore index:5
2023-03-01 14:53:11,479 - __main__ - INFO - rescore index:1
2023-03-01 14:53:19,672 - __main__ - INFO - rescore index:2
2023-03-01 14:53:20,318 - __main__ - INFO - rescore index:5
2023-03-01 14:53:24,871 - __main__ - INFO - rescore index:6
2023-03-01 14:53:35,007 - __main__ - INFO - rescore index:1
2023-03-01 14:53:36,937 - __main__ - INFO - rescore index:4
2023-03-01 14:53:37,597 - __main__ - INFO - rescore index:1
2023-03-01 14:53:40,199 - __main__ - INFO - rescore index:1
2023-03-01 14:53:40,857 - __main__ - INFO - rescore index:2
2023-03-01 14:53:42,800 - __main__ - INFO - rescore index:1
2023-03-01 14:53:44,102 - __main__ - INFO - rescore index:1
2023-03-01 14:53:44,616 - __main__ - INFO - 

2023-03-01 14:53:44,616 - __main__ - INFO - evalWER:219,evalWCount:625
2023-03-01 14:53:44,616 - __main__ - INFO - batch1 || Test CER: 0.23445 || Test WER: 0.35040
2023-03-01 15:00:28,539 - __main__ - INFO - rescore index:1
2023-03-01 15:00:32,328 - __main__ - INFO - rescore index:2
2023-03-01 15:00:34,266 - __main__ - INFO - rescore index:1
2023-03-01 15:00:36,613 - __main__ - INFO - rescore index:6
2023-03-01 15:00:40,480 - __main__ - INFO - rescore index:1
2023-03-01 15:00:42,425 - __main__ - INFO - rescore index:1
2023-03-01 15:00:51,270 - __main__ - INFO - rescore index:2
2023-03-01 15:00:56,429 - __main__ - INFO - rescore index:1
2023-03-01 15:00:58,375 - __main__ - INFO - rescore index:4
2023-03-01 15:00:59,021 - __main__ - INFO - rescore index:1
2023-03-01 15:00:59,671 - __main__ - INFO - rescore index:1
2023-03-01 15:01:07,044 - __main__ - INFO - 

2023-03-01 15:01:07,045 - __main__ - INFO - evalWER:438,evalWCount:1084
2023-03-01 15:01:07,045 - __main__ - INFO - batch2 || Test CER: 0.27155 || Test WER: 0.40406
2023-03-01 15:03:56,034 - __main__ - INFO - Namespace(batch_size=64, beam=500, beamWidth=20, beamsizetoken=None, beamthreshold=100.0, beta=0.1, decode_type='HYBRID_RESCORE', eval_lrs3_model_file='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt', lexicon='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/lst/LRS23.lst', lmpath='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/LRS23_4gram.bin', lmweight=1, logname='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main_noneed/beam20/vo_0.1.txt', modal='VO', nbest=30, silweight=0, type='kenlm', unitlm=False, unkweight=-inf, wordscore=2)
2023-03-01 15:03:56,034 - __main__ - INFO - 
Trained Model File: /home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt
2023-03-01 15:03:56,034 - __main__ - INFO - no noise
2023-03-01 15:04:00,788 - __main__ - INFO - _IncompatibleKeys(missing_keys=['transformer_lm._float_tensor', 'transformer_lm.models.0.decoder.version', 'transformer_lm.models.0.decoder.embed_tokens.weight', 'transformer_lm.models.0.decoder.project_in_dim.weight', 'transformer_lm.models.0.decoder.embed_positions._float_tensor', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.0.fc1.weight', 'transformer_lm.models.0.decoder.layers.0.fc1.bias', 'transformer_lm.models.0.decoder.layers.0.fc2.weight', 'transformer_lm.models.0.decoder.layers.0.fc2.bias', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.fc1.weight', 'transformer_lm.models.0.decoder.layers.1.fc1.bias', 'transformer_lm.models.0.decoder.layers.1.fc2.weight', 'transformer_lm.models.0.decoder.layers.1.fc2.bias', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.fc1.weight', 'transformer_lm.models.0.decoder.layers.2.fc1.bias', 'transformer_lm.models.0.decoder.layers.2.fc2.weight', 'transformer_lm.models.0.decoder.layers.2.fc2.bias', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.fc1.weight', 'transformer_lm.models.0.decoder.layers.3.fc1.bias', 'transformer_lm.models.0.decoder.layers.3.fc2.weight', 'transformer_lm.models.0.decoder.layers.3.fc2.bias', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.fc1.weight', 'transformer_lm.models.0.decoder.layers.4.fc1.bias', 'transformer_lm.models.0.decoder.layers.4.fc2.weight', 'transformer_lm.models.0.decoder.layers.4.fc2.bias', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.fc1.weight', 'transformer_lm.models.0.decoder.layers.5.fc1.bias', 'transformer_lm.models.0.decoder.layers.5.fc2.weight', 'transformer_lm.models.0.decoder.layers.5.fc2.bias', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.fc1.weight', 'transformer_lm.models.0.decoder.layers.6.fc1.bias', 'transformer_lm.models.0.decoder.layers.6.fc2.weight', 'transformer_lm.models.0.decoder.layers.6.fc2.bias', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.fc1.weight', 'transformer_lm.models.0.decoder.layers.7.fc1.bias', 'transformer_lm.models.0.decoder.layers.7.fc2.weight', 'transformer_lm.models.0.decoder.layers.7.fc2.bias', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.fc1.weight', 'transformer_lm.models.0.decoder.layers.8.fc1.bias', 'transformer_lm.models.0.decoder.layers.8.fc2.weight', 'transformer_lm.models.0.decoder.layers.8.fc2.bias', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.fc1.weight', 'transformer_lm.models.0.decoder.layers.9.fc1.bias', 'transformer_lm.models.0.decoder.layers.9.fc2.weight', 'transformer_lm.models.0.decoder.layers.9.fc2.bias', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.fc1.weight', 'transformer_lm.models.0.decoder.layers.10.fc1.bias', 'transformer_lm.models.0.decoder.layers.10.fc2.weight', 'transformer_lm.models.0.decoder.layers.10.fc2.bias', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.fc1.weight', 'transformer_lm.models.0.decoder.layers.11.fc1.bias', 'transformer_lm.models.0.decoder.layers.11.fc2.weight', 'transformer_lm.models.0.decoder.layers.11.fc2.bias', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.bias', 'transformer_lm.models.0.decoder.project_out_dim.weight', 'transformer_lm.models.0.decoder.output_projection.weight'], unexpected_keys=[])
2023-03-01 15:04:00,947 - __main__ - INFO - 
Testing the trained model .... 

2023-03-01 15:04:00,947 - __main__ - INFO - beamWidth:20.000000
2023-03-01 15:11:18,812 - __main__ - INFO - rescore index:1
2023-03-01 15:11:21,931 - __main__ - INFO - rescore index:5
2023-03-01 15:11:26,984 - __main__ - INFO - rescore index:1
2023-03-01 15:11:34,967 - __main__ - INFO - rescore index:2
2023-03-01 15:11:35,598 - __main__ - INFO - rescore index:5
2023-03-01 15:11:40,000 - __main__ - INFO - rescore index:6
2023-03-01 15:11:49,886 - __main__ - INFO - rescore index:1
2023-03-01 15:11:51,758 - __main__ - INFO - rescore index:4
2023-03-01 15:11:52,392 - __main__ - INFO - rescore index:1
2023-03-01 15:11:54,902 - __main__ - INFO - rescore index:1
2023-03-01 15:11:55,540 - __main__ - INFO - rescore index:2
2023-03-01 15:11:57,421 - __main__ - INFO - rescore index:1
2023-03-01 15:11:58,679 - __main__ - INFO - rescore index:1
2023-03-01 15:11:59,135 - __main__ - INFO - 

2023-03-01 15:11:59,135 - __main__ - INFO - evalWER:219,evalWCount:625
2023-03-01 15:11:59,135 - __main__ - INFO - batch1 || Test CER: 0.23445 || Test WER: 0.35040
2023-03-01 15:18:59,580 - __main__ - INFO - rescore index:1
2023-03-01 15:19:03,323 - __main__ - INFO - rescore index:2
2023-03-01 15:19:05,208 - __main__ - INFO - rescore index:1
2023-03-01 15:19:07,594 - __main__ - INFO - rescore index:6
2023-03-01 15:19:11,324 - __main__ - INFO - rescore index:1
2023-03-01 15:19:13,190 - __main__ - INFO - rescore index:1
2023-03-01 15:19:21,806 - __main__ - INFO - rescore index:2
2023-03-01 15:19:26,786 - __main__ - INFO - rescore index:1
2023-03-01 15:19:28,664 - __main__ - INFO - rescore index:4
2023-03-01 15:19:29,287 - __main__ - INFO - rescore index:1
2023-03-01 15:19:29,913 - __main__ - INFO - rescore index:1
2023-03-01 15:19:37,149 - __main__ - INFO - 

2023-03-01 15:19:37,149 - __main__ - INFO - evalWER:438,evalWCount:1084
2023-03-01 15:19:37,149 - __main__ - INFO - batch2 || Test CER: 0.27155 || Test WER: 0.40406
2023-03-01 15:26:33,404 - __main__ - INFO - rescore index:1
2023-03-01 15:26:35,274 - __main__ - INFO - rescore index:9
2023-03-01 15:26:37,773 - __main__ - INFO - rescore index:3
2023-03-01 15:26:39,035 - __main__ - INFO - rescore index:2
2023-03-01 15:26:39,657 - __main__ - INFO - rescore index:2
2023-03-01 15:26:43,885 - __main__ - INFO - rescore index:1
2023-03-01 15:26:45,122 - __main__ - INFO - rescore index:1
2023-03-01 15:26:51,344 - __main__ - INFO - rescore index:2
2023-03-01 15:26:53,217 - __main__ - INFO - rescore index:1
2023-03-01 15:26:55,729 - __main__ - INFO - rescore index:3
2023-03-01 15:26:58,092 - __main__ - INFO - rescore index:1
2023-03-01 15:26:58,716 - __main__ - INFO - rescore index:1
2023-03-01 15:27:04,311 - __main__ - INFO - rescore index:3
2023-03-01 15:27:09,317 - __main__ - INFO - rescore index:2
2023-03-01 15:27:12,150 - __main__ - INFO - 

2023-03-01 15:27:12,151 - __main__ - INFO - evalWER:681,evalWCount:1518
2023-03-01 15:27:12,151 - __main__ - INFO - batch3 || Test CER: 0.30155 || Test WER: 0.44862
2023-03-01 15:34:26,259 - __main__ - INFO - rescore index:1
2023-03-01 15:34:27,511 - __main__ - INFO - rescore index:7
2023-03-01 15:34:28,140 - __main__ - INFO - rescore index:1
2023-03-01 15:34:29,392 - __main__ - INFO - rescore index:1
2023-03-01 15:34:30,657 - __main__ - INFO - rescore index:21
2023-03-01 15:34:31,302 - __main__ - INFO - rescore index:4
2023-03-01 15:34:33,189 - __main__ - INFO - rescore index:1
2023-03-01 15:34:37,446 - __main__ - INFO - rescore index:3
2023-03-01 15:34:38,700 - __main__ - INFO - rescore index:1
2023-03-01 15:34:41,821 - __main__ - INFO - rescore index:1
2023-03-01 15:34:43,711 - __main__ - INFO - rescore index:2
2023-03-01 15:34:48,117 - __main__ - INFO - rescore index:1
2023-03-01 15:34:51,140 - __main__ - INFO - rescore index:1
2023-03-01 15:34:53,629 - __main__ - INFO - rescore index:2
2023-03-01 15:34:56,746 - __main__ - INFO - rescore index:2
2023-03-01 15:34:57,996 - __main__ - INFO - rescore index:2
2023-03-01 15:34:59,256 - __main__ - INFO - rescore index:1
2023-03-01 15:35:01,138 - __main__ - INFO - rescore index:1
2023-03-01 15:35:01,626 - __main__ - INFO - 

2023-03-01 15:35:01,626 - __main__ - INFO - evalWER:947,evalWCount:2021
2023-03-01 15:35:01,626 - __main__ - INFO - batch4 || Test CER: 0.31837 || Test WER: 0.46858
2023-03-01 15:42:27,439 - __main__ - INFO - rescore index:4
2023-03-01 15:42:38,100 - __main__ - INFO - rescore index:3
2023-03-01 15:42:38,728 - __main__ - INFO - rescore index:1
2023-03-01 15:42:40,464 - __main__ - INFO - rescore index:4
2023-03-01 15:42:41,714 - __main__ - INFO - rescore index:1
2023-03-01 15:42:44,227 - __main__ - INFO - rescore index:1
2023-03-01 15:42:48,622 - __main__ - INFO - rescore index:8
2023-03-01 15:42:49,868 - __main__ - INFO - rescore index:4
2023-03-01 15:42:52,383 - __main__ - INFO - rescore index:1
2023-03-01 15:42:54,803 - __main__ - INFO - rescore index:3
2023-03-01 15:42:58,547 - __main__ - INFO - rescore index:1
2023-03-01 15:43:00,435 - __main__ - INFO - rescore index:3
2023-03-01 15:43:02,943 - __main__ - INFO - rescore index:2
2023-03-01 15:43:06,692 - __main__ - INFO - rescore index:2
2023-03-01 15:43:07,209 - __main__ - INFO - 

2023-03-01 15:43:07,209 - __main__ - INFO - evalWER:1175,evalWCount:2599
2023-03-01 15:43:07,209 - __main__ - INFO - batch5 || Test CER: 0.30714 || Test WER: 0.45210
2023-03-01 15:50:22,413 - __main__ - INFO - rescore index:2
2023-03-01 15:50:23,655 - __main__ - INFO - rescore index:2
2023-03-01 15:50:25,530 - __main__ - INFO - rescore index:4
2023-03-01 15:50:27,401 - __main__ - INFO - rescore index:1
2023-03-01 15:50:28,649 - __main__ - INFO - rescore index:2
2023-03-01 15:50:31,630 - __main__ - INFO - rescore index:1
2023-03-01 15:50:34,729 - __main__ - INFO - rescore index:1
2023-03-01 15:50:38,489 - __main__ - INFO - rescore index:1
2023-03-01 15:50:40,366 - __main__ - INFO - rescore index:1
2023-03-01 15:50:40,994 - __main__ - INFO - rescore index:18
2023-03-01 15:50:44,128 - __main__ - INFO - rescore index:1
2023-03-01 15:50:46,510 - __main__ - INFO - rescore index:1
2023-03-01 15:50:49,007 - __main__ - INFO - rescore index:1
2023-03-01 15:50:57,124 - __main__ - INFO - rescore index:5
2023-03-01 15:51:03,716 - __main__ - INFO - 

2023-03-01 15:51:03,716 - __main__ - INFO - evalWER:1375,evalWCount:3029
2023-03-01 15:51:03,716 - __main__ - INFO - batch6 || Test CER: 0.30749 || Test WER: 0.45395
2023-03-01 15:58:26,038 - __main__ - INFO - rescore index:1
2023-03-01 15:58:29,593 - __main__ - INFO - rescore index:2
2023-03-01 15:58:31,491 - __main__ - INFO - rescore index:1
2023-03-01 15:58:32,130 - __main__ - INFO - rescore index:13
2023-03-01 15:58:34,039 - __main__ - INFO - rescore index:2
2023-03-01 15:58:41,059 - __main__ - INFO - rescore index:1
2023-03-01 15:58:42,972 - __main__ - INFO - rescore index:1
2023-03-01 15:58:44,704 - __main__ - INFO - rescore index:1
2023-03-01 15:58:45,338 - __main__ - INFO - rescore index:4
2023-03-01 15:58:48,498 - __main__ - INFO - rescore index:6
2023-03-01 15:58:51,685 - __main__ - INFO - rescore index:2
2023-03-01 15:58:54,876 - __main__ - INFO - rescore index:1
2023-03-01 15:58:59,846 - __main__ - INFO - rescore index:2
2023-03-01 15:59:02,411 - __main__ - INFO - rescore index:2
2023-03-01 15:59:04,363 - __main__ - INFO - rescore index:2
2023-03-01 15:59:06,318 - __main__ - INFO - rescore index:2
2023-03-01 15:59:08,023 - __main__ - INFO - 

2023-03-01 15:59:08,023 - __main__ - INFO - evalWER:1586,evalWCount:3496
2023-03-01 15:59:08,023 - __main__ - INFO - batch7 || Test CER: 0.30868 || Test WER: 0.45366
2023-03-01 16:02:02,792 - __main__ - INFO - Namespace(batch_size=48, beam=500, beamWidth=20, beamsizetoken=None, beamthreshold=100.0, beta=0.1, decode_type='HYBRID_RESCORE', eval_lrs3_model_file='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt', lexicon='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/lst/LRS23.lst', lmpath='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/LRS23_4gram.bin', lmweight=1, logname='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main_noneed/beam20/vo_0.1.txt', modal='VO', nbest=30, silweight=0, type='kenlm', unitlm=False, unkweight=-inf, wordscore=2)
2023-03-01 16:02:02,793 - __main__ - INFO - 
Trained Model File: /home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt
2023-03-01 16:02:02,793 - __main__ - INFO - no noise
2023-03-01 16:02:07,935 - __main__ - INFO - _IncompatibleKeys(missing_keys=['transformer_lm._float_tensor', 'transformer_lm.models.0.decoder.version', 'transformer_lm.models.0.decoder.embed_tokens.weight', 'transformer_lm.models.0.decoder.project_in_dim.weight', 'transformer_lm.models.0.decoder.embed_positions._float_tensor', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.0.fc1.weight', 'transformer_lm.models.0.decoder.layers.0.fc1.bias', 'transformer_lm.models.0.decoder.layers.0.fc2.weight', 'transformer_lm.models.0.decoder.layers.0.fc2.bias', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.fc1.weight', 'transformer_lm.models.0.decoder.layers.1.fc1.bias', 'transformer_lm.models.0.decoder.layers.1.fc2.weight', 'transformer_lm.models.0.decoder.layers.1.fc2.bias', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.fc1.weight', 'transformer_lm.models.0.decoder.layers.2.fc1.bias', 'transformer_lm.models.0.decoder.layers.2.fc2.weight', 'transformer_lm.models.0.decoder.layers.2.fc2.bias', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.fc1.weight', 'transformer_lm.models.0.decoder.layers.3.fc1.bias', 'transformer_lm.models.0.decoder.layers.3.fc2.weight', 'transformer_lm.models.0.decoder.layers.3.fc2.bias', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.fc1.weight', 'transformer_lm.models.0.decoder.layers.4.fc1.bias', 'transformer_lm.models.0.decoder.layers.4.fc2.weight', 'transformer_lm.models.0.decoder.layers.4.fc2.bias', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.fc1.weight', 'transformer_lm.models.0.decoder.layers.5.fc1.bias', 'transformer_lm.models.0.decoder.layers.5.fc2.weight', 'transformer_lm.models.0.decoder.layers.5.fc2.bias', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.fc1.weight', 'transformer_lm.models.0.decoder.layers.6.fc1.bias', 'transformer_lm.models.0.decoder.layers.6.fc2.weight', 'transformer_lm.models.0.decoder.layers.6.fc2.bias', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.fc1.weight', 'transformer_lm.models.0.decoder.layers.7.fc1.bias', 'transformer_lm.models.0.decoder.layers.7.fc2.weight', 'transformer_lm.models.0.decoder.layers.7.fc2.bias', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.fc1.weight', 'transformer_lm.models.0.decoder.layers.8.fc1.bias', 'transformer_lm.models.0.decoder.layers.8.fc2.weight', 'transformer_lm.models.0.decoder.layers.8.fc2.bias', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.fc1.weight', 'transformer_lm.models.0.decoder.layers.9.fc1.bias', 'transformer_lm.models.0.decoder.layers.9.fc2.weight', 'transformer_lm.models.0.decoder.layers.9.fc2.bias', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.fc1.weight', 'transformer_lm.models.0.decoder.layers.10.fc1.bias', 'transformer_lm.models.0.decoder.layers.10.fc2.weight', 'transformer_lm.models.0.decoder.layers.10.fc2.bias', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.fc1.weight', 'transformer_lm.models.0.decoder.layers.11.fc1.bias', 'transformer_lm.models.0.decoder.layers.11.fc2.weight', 'transformer_lm.models.0.decoder.layers.11.fc2.bias', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.bias', 'transformer_lm.models.0.decoder.project_out_dim.weight', 'transformer_lm.models.0.decoder.output_projection.weight'], unexpected_keys=[])
2023-03-01 16:02:08,038 - __main__ - INFO - 
Testing the trained model .... 

2023-03-01 16:02:08,039 - __main__ - INFO - beamWidth:20.000000
2023-03-01 16:05:15,340 - __main__ - INFO - Namespace(batch_size=48, beam=500, beamWidth=20, beamsizetoken=None, beamthreshold=100.0, beta=0.1, decode_type='HYBRID_RESCORE', eval_lrs3_model_file='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt', lexicon='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/lst/LRS23.lst', lmpath='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/LRS23_4gram.bin', lmweight=1, logname='/home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main_noneed/beam20/vo_0.1.txt', modal='VO', nbest=30, silweight=0, type='kenlm', unitlm=False, unkweight=-inf, wordscore=2)
2023-03-01 16:05:15,340 - __main__ - INFO - 
Trained Model File: /home/gryang/Leveraging-Self-Supervised-Learning-for-AVSR-main/check/train-step_1191-wer_0.674.ckpt
2023-03-01 16:05:15,341 - __main__ - INFO - no noise
2023-03-01 16:05:19,586 - __main__ - INFO - _IncompatibleKeys(missing_keys=['transformer_lm._float_tensor', 'transformer_lm.models.0.decoder.version', 'transformer_lm.models.0.decoder.embed_tokens.weight', 'transformer_lm.models.0.decoder.project_in_dim.weight', 'transformer_lm.models.0.decoder.embed_positions._float_tensor', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.0.fc1.weight', 'transformer_lm.models.0.decoder.layers.0.fc1.bias', 'transformer_lm.models.0.decoder.layers.0.fc2.weight', 'transformer_lm.models.0.decoder.layers.0.fc2.bias', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.0.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.1.fc1.weight', 'transformer_lm.models.0.decoder.layers.1.fc1.bias', 'transformer_lm.models.0.decoder.layers.1.fc2.weight', 'transformer_lm.models.0.decoder.layers.1.fc2.bias', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.1.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.2.fc1.weight', 'transformer_lm.models.0.decoder.layers.2.fc1.bias', 'transformer_lm.models.0.decoder.layers.2.fc2.weight', 'transformer_lm.models.0.decoder.layers.2.fc2.bias', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.2.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.3.fc1.weight', 'transformer_lm.models.0.decoder.layers.3.fc1.bias', 'transformer_lm.models.0.decoder.layers.3.fc2.weight', 'transformer_lm.models.0.decoder.layers.3.fc2.bias', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.3.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.4.fc1.weight', 'transformer_lm.models.0.decoder.layers.4.fc1.bias', 'transformer_lm.models.0.decoder.layers.4.fc2.weight', 'transformer_lm.models.0.decoder.layers.4.fc2.bias', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.4.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.5.fc1.weight', 'transformer_lm.models.0.decoder.layers.5.fc1.bias', 'transformer_lm.models.0.decoder.layers.5.fc2.weight', 'transformer_lm.models.0.decoder.layers.5.fc2.bias', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.5.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.6.fc1.weight', 'transformer_lm.models.0.decoder.layers.6.fc1.bias', 'transformer_lm.models.0.decoder.layers.6.fc2.weight', 'transformer_lm.models.0.decoder.layers.6.fc2.bias', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.6.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.7.fc1.weight', 'transformer_lm.models.0.decoder.layers.7.fc1.bias', 'transformer_lm.models.0.decoder.layers.7.fc2.weight', 'transformer_lm.models.0.decoder.layers.7.fc2.bias', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.7.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.8.fc1.weight', 'transformer_lm.models.0.decoder.layers.8.fc1.bias', 'transformer_lm.models.0.decoder.layers.8.fc2.weight', 'transformer_lm.models.0.decoder.layers.8.fc2.bias', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.8.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.9.fc1.weight', 'transformer_lm.models.0.decoder.layers.9.fc1.bias', 'transformer_lm.models.0.decoder.layers.9.fc2.weight', 'transformer_lm.models.0.decoder.layers.9.fc2.bias', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.9.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.10.fc1.weight', 'transformer_lm.models.0.decoder.layers.10.fc1.bias', 'transformer_lm.models.0.decoder.layers.10.fc2.weight', 'transformer_lm.models.0.decoder.layers.10.fc2.bias', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.10.final_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.k_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.v_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.q_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn.out_proj.bias', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.self_attn_layer_norm.bias', 'transformer_lm.models.0.decoder.layers.11.fc1.weight', 'transformer_lm.models.0.decoder.layers.11.fc1.bias', 'transformer_lm.models.0.decoder.layers.11.fc2.weight', 'transformer_lm.models.0.decoder.layers.11.fc2.bias', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.weight', 'transformer_lm.models.0.decoder.layers.11.final_layer_norm.bias', 'transformer_lm.models.0.decoder.project_out_dim.weight', 'transformer_lm.models.0.decoder.output_projection.weight'], unexpected_keys=[])
2023-03-01 16:05:19,673 - __main__ - INFO - 
Testing the trained model .... 

2023-03-01 16:05:19,673 - __main__ - INFO - beamWidth:20.000000
2023-03-01 16:10:14,973 - __main__ - INFO - rescore index:1
2023-03-01 16:10:18,451 - __main__ - INFO - rescore index:5
2023-03-01 16:10:23,650 - __main__ - INFO - rescore index:1
2023-03-01 16:10:31,827 - __main__ - INFO - rescore index:2
2023-03-01 16:10:32,482 - __main__ - INFO - rescore index:5
2023-03-01 16:10:37,023 - __main__ - INFO - rescore index:6
2023-03-01 16:10:45,972 - __main__ - INFO - 

2023-03-01 16:10:45,972 - __main__ - INFO - evalWER:165,evalWCount:484
2023-03-01 16:10:45,972 - __main__ - INFO - batch1 || Test CER: 0.23493 || Test WER: 0.34091
2023-03-01 16:15:33,441 - __main__ - INFO - rescore index:1
2023-03-01 16:15:35,376 - __main__ - INFO - rescore index:4
2023-03-01 16:15:36,032 - __main__ - INFO - rescore index:1
2023-03-01 16:15:38,626 - __main__ - INFO - rescore index:1
2023-03-01 16:15:39,281 - __main__ - INFO - rescore index:2
2023-03-01 16:15:41,238 - __main__ - INFO - rescore index:1
2023-03-01 16:15:42,544 - __main__ - INFO - rescore index:1
2023-03-01 16:15:47,386 - __main__ - INFO - rescore index:1
2023-03-01 16:15:51,259 - __main__ - INFO - rescore index:2
2023-03-01 16:15:53,198 - __main__ - INFO - rescore index:1
2023-03-01 16:15:55,149 - __main__ - INFO - rescore index:6
2023-03-01 16:15:59,302 - __main__ - INFO - rescore index:1
2023-03-01 16:16:01,256 - __main__ - INFO - rescore index:1
2023-03-01 16:16:04,101 - __main__ - INFO - 

2023-03-01 16:16:04,101 - __main__ - INFO - evalWER:313,evalWCount:834
2023-03-01 16:16:04,101 - __main__ - INFO - batch2 || Test CER: 0.24929 || Test WER: 0.37530
2023-03-01 16:20:45,017 - __main__ - INFO - rescore index:2
2023-03-01 16:20:50,045 - __main__ - INFO - rescore index:1
2023-03-01 16:20:52,294 - __main__ - INFO - rescore index:4
2023-03-01 16:20:52,917 - __main__ - INFO - rescore index:1
2023-03-01 16:20:53,540 - __main__ - INFO - rescore index:1
2023-03-01 16:21:03,408 - __main__ - INFO - rescore index:1
2023-03-01 16:21:05,280 - __main__ - INFO - rescore index:9
2023-03-01 16:21:07,782 - __main__ - INFO - rescore index:3
2023-03-01 16:21:09,041 - __main__ - INFO - rescore index:2
2023-03-01 16:21:09,667 - __main__ - INFO - rescore index:2
2023-03-01 16:21:10,597 - __main__ - INFO - 

2023-03-01 16:21:10,598 - __main__ - INFO - evalWER:498,evalWCount:1188
2023-03-01 16:21:10,598 - __main__ - INFO - batch3 || Test CER: 0.28093 || Test WER: 0.41919
2023-03-01 16:25:53,038 - __main__ - INFO - rescore index:1
2023-03-01 16:25:54,285 - __main__ - INFO - rescore index:1
2023-03-01 16:26:00,841 - __main__ - INFO - rescore index:2
2023-03-01 16:26:02,729 - __main__ - INFO - rescore index:1
2023-03-01 16:26:05,253 - __main__ - INFO - rescore index:3
2023-03-01 16:26:07,484 - __main__ - INFO - rescore index:1
2023-03-01 16:26:08,111 - __main__ - INFO - rescore index:1
2023-03-01 16:26:13,772 - __main__ - INFO - rescore index:3
2023-03-01 16:26:19,154 - __main__ - INFO - rescore index:2
2023-03-01 16:26:21,328 - __main__ - INFO - 

2023-03-01 16:26:21,328 - __main__ - INFO - evalWER:681,evalWCount:1518
2023-03-01 16:26:21,328 - __main__ - INFO - batch4 || Test CER: 0.30155 || Test WER: 0.44862
2023-03-01 16:31:16,083 - __main__ - INFO - rescore index:1
2023-03-01 16:31:17,406 - __main__ - INFO - rescore index:7
2023-03-01 16:31:18,098 - __main__ - INFO - rescore index:1
2023-03-01 16:31:19,391 - __main__ - INFO - rescore index:1
2023-03-01 16:31:20,700 - __main__ - INFO - rescore index:21
2023-03-01 16:31:21,356 - __main__ - INFO - rescore index:4
2023-03-01 16:31:23,307 - __main__ - INFO - rescore index:1
2023-03-01 16:31:27,528 - __main__ - INFO - rescore index:3
2023-03-01 16:31:28,815 - __main__ - INFO - rescore index:1
2023-03-01 16:31:32,063 - __main__ - INFO - rescore index:1
2023-03-01 16:31:34,036 - __main__ - INFO - rescore index:2
2023-03-01 16:31:38,633 - __main__ - INFO - rescore index:1
2023-03-01 16:31:41,519 - __main__ - INFO - rescore index:1
2023-03-01 16:31:41,783 - __main__ - INFO - 

2023-03-01 16:31:41,783 - __main__ - INFO - evalWER:891,evalWCount:1908
2023-03-01 16:31:41,783 - __main__ - INFO - batch5 || Test CER: 0.31575 || Test WER: 0.46698
2023-03-01 16:36:35,004 - __main__ - INFO - rescore index:2
2023-03-01 16:36:38,233 - __main__ - INFO - rescore index:2
2023-03-01 16:36:39,524 - __main__ - INFO - rescore index:2
2023-03-01 16:36:41,088 - __main__ - INFO - rescore index:1
2023-03-01 16:36:43,014 - __main__ - INFO - rescore index:1
2023-03-01 16:36:44,960 - __main__ - INFO - rescore index:4
2023-03-01 16:36:56,295 - __main__ - INFO - rescore index:3
2023-03-01 16:36:56,940 - __main__ - INFO - rescore index:1
2023-03-01 16:36:58,229 - __main__ - INFO - rescore index:4
2023-03-01 16:36:59,532 - __main__ - INFO - rescore index:1
2023-03-01 16:37:02,137 - __main__ - INFO - rescore index:1
2023-03-01 16:37:04,641 - __main__ - INFO - 

2023-03-01 16:37:04,642 - __main__ - INFO - evalWER:1072,evalWCount:2310
2023-03-01 16:37:04,642 - __main__ - INFO - batch6 || Test CER: 0.31533 || Test WER: 0.46407
2023-03-01 16:42:02,738 - __main__ - INFO - rescore index:8
2023-03-01 16:42:03,991 - __main__ - INFO - rescore index:4
2023-03-01 16:42:06,811 - __main__ - INFO - rescore index:1
2023-03-01 16:42:08,690 - __main__ - INFO - rescore index:3
2023-03-01 16:42:12,459 - __main__ - INFO - rescore index:1
2023-03-01 16:42:14,355 - __main__ - INFO - rescore index:3
2023-03-01 16:42:17,226 - __main__ - INFO - rescore index:2
2023-03-01 16:42:20,969 - __main__ - INFO - rescore index:2
2023-03-01 16:42:21,610 - __main__ - INFO - rescore index:2
2023-03-01 16:42:22,860 - __main__ - INFO - rescore index:2
2023-03-01 16:42:24,758 - __main__ - INFO - rescore index:4
2023-03-01 16:42:26,634 - __main__ - INFO - rescore index:1
2023-03-01 16:42:28,267 - __main__ - INFO - rescore index:2
2023-03-01 16:42:30,758 - __main__ - INFO - rescore index:1
2023-03-01 16:42:31,684 - __main__ - INFO - 

2023-03-01 16:42:31,685 - __main__ - INFO - evalWER:1217,evalWCount:2719
2023-03-01 16:42:31,685 - __main__ - INFO - batch7 || Test CER: 0.30290 || Test WER: 0.44759
2023-03-01 16:47:15,988 - __main__ - INFO - rescore index:1
2023-03-01 16:47:19,880 - __main__ - INFO - rescore index:1
2023-03-01 16:47:21,798 - __main__ - INFO - rescore index:1
2023-03-01 16:47:22,426 - __main__ - INFO - rescore index:18
2023-03-01 16:47:25,912 - __main__ - INFO - rescore index:1
2023-03-01 16:47:27,788 - __main__ - INFO - rescore index:1
2023-03-01 16:47:30,320 - __main__ - INFO - rescore index:1
2023-03-01 16:47:38,990 - __main__ - INFO - rescore index:5
2023-03-01 16:47:44,943 - __main__ - INFO - 

2023-03-01 16:47:44,943 - __main__ - INFO - evalWER:1375,evalWCount:3029
2023-03-01 16:47:44,943 - __main__ - INFO - batch8 || Test CER: 0.30749 || Test WER: 0.45395
2023-03-01 16:52:38,171 - __main__ - INFO - rescore index:1
2023-03-01 16:52:41,298 - __main__ - INFO - rescore index:2
2023-03-01 16:52:43,187 - __main__ - INFO - rescore index:1
2023-03-01 16:52:43,828 - __main__ - INFO - rescore index:13
2023-03-01 16:52:46,047 - __main__ - INFO - rescore index:2
2023-03-01 16:52:52,943 - __main__ - INFO - rescore index:1
2023-03-01 16:52:54,835 - __main__ - INFO - rescore index:1
2023-03-01 16:52:56,095 - __main__ - INFO - rescore index:1
2023-03-01 16:52:57,087 - __main__ - INFO - rescore index:4
2023-03-01 16:53:00,203 - __main__ - INFO - rescore index:6
2023-03-01 16:53:03,347 - __main__ - INFO - rescore index:2
2023-03-01 16:53:06,516 - __main__ - INFO - rescore index:1
2023-03-01 16:53:09,080 - __main__ - INFO - 

2023-03-01 16:53:09,080 - __main__ - INFO - evalWER:1549,evalWCount:3390
2023-03-01 16:53:09,080 - __main__ - INFO - batch9 || Test CER: 0.31079 || Test WER: 0.45693
2023-03-01 16:57:42,073 - __main__ - INFO - rescore index:2
2023-03-01 16:57:44,928 - __main__ - INFO - rescore index:2
2023-03-01 16:57:46,794 - __main__ - INFO - rescore index:2
2023-03-01 16:57:48,668 - __main__ - INFO - rescore index:2
2023-03-01 16:57:54,322 - __main__ - INFO - rescore index:1
2023-03-01 16:57:54,955 - __main__ - INFO - rescore index:25
2023-03-01 16:57:55,936 - __main__ - INFO - rescore index:2
2023-03-01 16:57:59,051 - __main__ - INFO - rescore index:1
2023-03-01 16:57:59,674 - __main__ - INFO - rescore index:1
2023-03-01 16:58:02,183 - __main__ - INFO - rescore index:1
2023-03-01 16:58:04,689 - __main__ - INFO - rescore index:1
2023-03-01 16:58:05,949 - __main__ - INFO - rescore index:1
2023-03-01 16:58:10,077 - __main__ - INFO - rescore index:3
2023-03-01 16:58:10,702 - __main__ - INFO - rescore index:1
2023-03-01 16:58:10,974 - __main__ - INFO - 

2023-03-01 16:58:10,974 - __main__ - INFO - evalWER:1702,evalWCount:3695
2023-03-01 16:58:10,974 - __main__ - INFO - batch10 || Test CER: 0.31248 || Test WER: 0.46062
2023-03-01 17:03:11,398 - __main__ - INFO - rescore index:1
2023-03-01 17:03:12,666 - __main__ - INFO - rescore index:1
2023-03-01 17:03:13,929 - __main__ - INFO - rescore index:3
2023-03-01 17:03:15,215 - __main__ - INFO - rescore index:1
2023-03-01 17:03:15,851 - __main__ - INFO - rescore index:1
2023-03-01 17:03:16,834 - __main__ - INFO - rescore index:1
2023-03-01 17:03:18,725 - __main__ - INFO - rescore index:1
2023-03-01 17:03:21,851 - __main__ - INFO - rescore index:1
2023-03-01 17:03:23,109 - __main__ - INFO - rescore index:20
2023-03-01 17:03:23,739 - __main__ - INFO - rescore index:1
2023-03-01 17:03:25,634 - __main__ - INFO - rescore index:2
2023-03-01 17:03:26,265 - __main__ - INFO - rescore index:1
2023-03-01 17:03:27,533 - __main__ - INFO - rescore index:1
2023-03-01 17:03:29,223 - __main__ - INFO - rescore index:1
2023-03-01 17:03:31,089 - __main__ - INFO - rescore index:1
2023-03-01 17:03:32,956 - __main__ - INFO - rescore index:2
2023-03-01 17:03:37,338 - __main__ - INFO - rescore index:1
2023-03-01 17:03:38,893 - __main__ - INFO - 

2023-03-01 17:03:38,893 - __main__ - INFO - evalWER:1844,evalWCount:3997
2023-03-01 17:03:38,893 - __main__ - INFO - batch11 || Test CER: 0.31453 || Test WER: 0.46135
2023-03-01 17:07:56,045 - __main__ - INFO - rescore index:1
2023-03-01 17:07:56,687 - __main__ - INFO - rescore index:4
2023-03-01 17:07:58,610 - __main__ - INFO - rescore index:8
2023-03-01 17:08:01,192 - __main__ - INFO - rescore index:1
2023-03-01 17:08:07,254 - __main__ - INFO - rescore index:5
2023-03-01 17:08:07,894 - __main__ - INFO - rescore index:1
2023-03-01 17:08:08,556 - __main__ - INFO - rescore index:1
2023-03-01 17:08:11,140 - __main__ - INFO - rescore index:3
2023-03-01 17:08:15,309 - __main__ - INFO - rescore index:1
2023-03-01 17:08:17,235 - __main__ - INFO - rescore index:1
2023-03-01 17:08:17,457 - __main__ - INFO - 

2023-03-01 17:08:17,458 - __main__ - INFO - evalWER:1942,evalWCount:4307
2023-03-01 17:08:17,458 - __main__ - INFO - batch12 || Test CER: 0.30661 || Test WER: 0.45089
2023-03-01 17:13:12,982 - __main__ - INFO - rescore index:2
2023-03-01 17:13:17,734 - __main__ - INFO - rescore index:2
2023-03-01 17:13:18,381 - __main__ - INFO - rescore index:1
2023-03-01 17:13:28,388 - __main__ - INFO - rescore index:2
2023-03-01 17:13:30,322 - __main__ - INFO - rescore index:1
2023-03-01 17:13:36,147 - __main__ - INFO - rescore index:2
2023-03-01 17:13:39,039 - __main__ - INFO - rescore index:3
2023-03-01 17:13:40,975 - __main__ - INFO - rescore index:1
2023-03-01 17:13:41,627 - __main__ - INFO - rescore index:1
2023-03-01 17:13:44,495 - __main__ - INFO - 

2023-03-01 17:13:44,495 - __main__ - INFO - evalWER:2084,evalWCount:4652
2023-03-01 17:13:44,495 - __main__ - INFO - batch13 || Test CER: 0.30333 || Test WER: 0.44798
2023-03-01 17:18:11,808 - __main__ - INFO - rescore index:4
2023-03-01 17:18:12,463 - __main__ - INFO - rescore index:2
2023-03-01 17:18:15,314 - __main__ - INFO - rescore index:10
2023-03-01 17:18:15,970 - __main__ - INFO - rescore index:4
2023-03-01 17:18:18,586 - __main__ - INFO - rescore index:9
2023-03-01 17:18:21,847 - __main__ - INFO - rescore index:2
2023-03-01 17:18:25,384 - __main__ - INFO - rescore index:3
2023-03-01 17:18:26,033 - __main__ - INFO - rescore index:4
2023-03-01 17:18:30,577 - __main__ - INFO - rescore index:1
2023-03-01 17:18:34,488 - __main__ - INFO - rescore index:1
2023-03-01 17:18:37,994 - __main__ - INFO - rescore index:2
2023-03-01 17:18:38,250 - __main__ - INFO - 

2023-03-01 17:18:38,250 - __main__ - INFO - evalWER:2284,evalWCount:5049
2023-03-01 17:18:38,251 - __main__ - INFO - batch14 || Test CER: 0.30564 || Test WER: 0.45237
2023-03-01 17:23:40,305 - __main__ - INFO - rescore index:2
2023-03-01 17:23:40,934 - __main__ - INFO - rescore index:4
2023-03-01 17:23:41,564 - __main__ - INFO - rescore index:1
2023-03-01 17:23:45,045 - __main__ - INFO - rescore index:1
2023-03-01 17:23:45,667 - __main__ - INFO - rescore index:1
2023-03-01 17:23:46,289 - __main__ - INFO - rescore index:1
2023-03-01 17:23:46,924 - __main__ - INFO - rescore index:2
2023-03-01 17:23:51,338 - __main__ - INFO - rescore index:1
2023-03-01 17:23:53,229 - __main__ - INFO - rescore index:2
2023-03-01 17:24:01,148 - __main__ - INFO - rescore index:1
2023-03-01 17:24:02,403 - __main__ - INFO - rescore index:1
2023-03-01 17:24:07,110 - __main__ - INFO - 

2023-03-01 17:24:07,110 - __main__ - INFO - evalWER:2472,evalWCount:5379
2023-03-01 17:24:07,110 - __main__ - INFO - batch15 || Test CER: 0.31052 || Test WER: 0.45956
2023-03-01 17:27:41,167 - __main__ - INFO - rescore index:1
2023-03-01 17:27:46,831 - __main__ - INFO - rescore index:3
2023-03-01 17:27:52,166 - __main__ - INFO - rescore index:1
2023-03-01 17:27:56,548 - __main__ - INFO - rescore index:2
2023-03-01 17:27:57,175 - __main__ - INFO - rescore index:2
2023-03-01 17:27:58,427 - __main__ - INFO - rescore index:5
2023-03-01 17:28:04,388 - __main__ - INFO - rescore index:4
2023-03-01 17:28:06,272 - __main__ - INFO - rescore index:1
2023-03-01 17:28:06,904 - __main__ - INFO - rescore index:2
2023-03-01 17:28:10,638 - __main__ - INFO - 

2023-03-01 17:28:10,639 - __main__ - INFO - evalWER:2653,evalWCount:5719
2023-03-01 17:28:10,639 - __main__ - INFO - batch16 || Test CER: 0.31330 || Test WER: 0.46389
2023-03-01 17:32:51,315 - __main__ - INFO - rescore index:1
2023-03-01 17:32:59,838 - __main__ - INFO - rescore index:1
2023-03-01 17:33:03,010 - __main__ - INFO - rescore index:1
2023-03-01 17:33:08,421 - __main__ - INFO - rescore index:1
2023-03-01 17:33:13,501 - __main__ - INFO - rescore index:1
2023-03-01 17:33:17,039 - __main__ - INFO - rescore index:9
2023-03-01 17:33:17,668 - __main__ - INFO - rescore index:5
2023-03-01 17:33:18,297 - __main__ - INFO - rescore index:1
2023-03-01 17:33:20,190 - __main__ - INFO - rescore index:5
2023-03-01 17:33:22,387 - __main__ - INFO - 

2023-03-01 17:33:22,387 - __main__ - INFO - evalWER:2814,evalWCount:6019
2023-03-01 17:33:22,387 - __main__ - INFO - batch17 || Test CER: 0.31508 || Test WER: 0.46752
2023-03-01 17:38:19,298 - __main__ - INFO - rescore index:2
2023-03-01 17:38:23,403 - __main__ - INFO - rescore index:3
2023-03-01 17:38:24,056 - __main__ - INFO - rescore index:2
2023-03-01 17:38:25,338 - __main__ - INFO - rescore index:1
2023-03-01 17:38:27,269 - __main__ - INFO - rescore index:2
2023-03-01 17:38:37,209 - __main__ - INFO - rescore index:1
2023-03-01 17:38:42,361 - __main__ - INFO - rescore index:1
2023-03-01 17:38:48,423 - __main__ - INFO - rescore index:1
2023-03-01 17:38:49,336 - __main__ - INFO - 

2023-03-01 17:38:49,336 - __main__ - INFO - evalWER:2983,evalWCount:6394
2023-03-01 17:38:49,336 - __main__ - INFO - batch18 || Test CER: 0.31333 || Test WER: 0.46653
2023-03-01 17:43:41,604 - __main__ - INFO - rescore index:6
2023-03-01 17:43:42,889 - __main__ - INFO - rescore index:3
2023-03-01 17:43:44,174 - __main__ - INFO - rescore index:1
2023-03-01 17:43:45,458 - __main__ - INFO - rescore index:1
2023-03-01 17:43:52,262 - __main__ - INFO - rescore index:1
2023-03-01 17:43:56,106 - __main__ - INFO - rescore index:1
2023-03-01 17:43:56,746 - __main__ - INFO - rescore index:2
2023-03-01 17:43:58,672 - __main__ - INFO - rescore index:1
2023-03-01 17:44:00,602 - __main__ - INFO - rescore index:1
2023-03-01 17:44:03,462 - __main__ - INFO - rescore index:10
2023-03-01 17:44:05,376 - __main__ - INFO - rescore index:1
2023-03-01 17:44:06,018 - __main__ - INFO - rescore index:8
2023-03-01 17:44:07,566 - __main__ - INFO - 

2023-03-01 17:44:07,566 - __main__ - INFO - evalWER:3137,evalWCount:6716
2023-03-01 17:44:07,567 - __main__ - INFO - batch19 || Test CER: 0.31278 || Test WER: 0.46709
2023-03-01 17:48:44,103 - __main__ - INFO - rescore index:24
2023-03-01 17:48:47,343 - __main__ - INFO - rescore index:3
2023-03-01 17:48:47,982 - __main__ - INFO - rescore index:2
2023-03-01 17:48:49,271 - __main__ - INFO - rescore index:1
2023-03-01 17:48:50,566 - __main__ - INFO - rescore index:2
2023-03-01 17:48:54,435 - __main__ - INFO - rescore index:6
2023-03-01 17:48:55,377 - __main__ - INFO - rescore index:5
2023-03-01 17:48:56,015 - __main__ - INFO - rescore index:28
2023-03-01 17:48:58,576 - __main__ - INFO - rescore index:10
2023-03-01 17:48:59,866 - __main__ - INFO - rescore index:1
2023-03-01 17:49:00,509 - __main__ - INFO - rescore index:2
2023-03-01 17:49:01,794 - __main__ - INFO - rescore index:2
2023-03-01 17:49:03,726 - __main__ - INFO - rescore index:1
2023-03-01 17:49:06,321 - __main__ - INFO - rescore index:2
2023-03-01 17:49:09,161 - __main__ - INFO - rescore index:3
2023-03-01 17:49:09,815 - __main__ - INFO - rescore index:3
2023-03-01 17:49:13,673 - __main__ - INFO - rescore index:2
2023-03-01 17:49:15,221 - __main__ - INFO - 

2023-03-01 17:49:15,222 - __main__ - INFO - evalWER:3371,evalWCount:7103
2023-03-01 17:49:15,222 - __main__ - INFO - batch20 || Test CER: 0.31844 || Test WER: 0.47459
2023-03-01 17:53:50,522 - __main__ - INFO - rescore index:1
2023-03-01 17:53:53,757 - __main__ - INFO - rescore index:5
2023-03-01 17:53:55,714 - __main__ - INFO - rescore index:1
2023-03-01 17:53:56,362 - __main__ - INFO - rescore index:3
2023-03-01 17:53:57,680 - __main__ - INFO - rescore index:15
2023-03-01 17:53:59,655 - __main__ - INFO - rescore index:17
2023-03-01 17:54:01,928 - __main__ - INFO - rescore index:5
2023-03-01 17:54:02,564 - __main__ - INFO - rescore index:1
2023-03-01 17:54:05,171 - __main__ - INFO - rescore index:2
2023-03-01 17:54:09,646 - __main__ - INFO - rescore index:1
2023-03-01 17:54:12,495 - __main__ - INFO - rescore index:1
2023-03-01 17:54:13,129 - __main__ - INFO - rescore index:1
2023-03-01 17:54:14,402 - __main__ - INFO - rescore index:1
2023-03-01 17:54:15,042 - __main__ - INFO - rescore index:1
2023-03-01 17:54:18,264 - __main__ - INFO - rescore index:4
2023-03-01 17:54:19,574 - __main__ - INFO - rescore index:10
2023-03-01 17:54:21,809 - __main__ - INFO - 

2023-03-01 17:54:21,809 - __main__ - INFO - evalWER:3547,evalWCount:7487
2023-03-01 17:54:21,809 - __main__ - INFO - batch21 || Test CER: 0.31849 || Test WER: 0.47375
2023-03-01 17:59:16,813 - __main__ - INFO - rescore index:2
2023-03-01 17:59:19,383 - __main__ - INFO - rescore index:2
2023-03-01 17:59:20,671 - __main__ - INFO - rescore index:14
2023-03-01 17:59:21,313 - __main__ - INFO - rescore index:1
2023-03-01 17:59:24,196 - __main__ - INFO - rescore index:1
2023-03-01 17:59:26,742 - __main__ - INFO - rescore index:2
2023-03-01 17:59:28,031 - __main__ - INFO - rescore index:1
2023-03-01 17:59:30,596 - __main__ - INFO - rescore index:1
2023-03-01 17:59:31,242 - __main__ - INFO - rescore index:11
2023-03-01 17:59:35,403 - __main__ - INFO - rescore index:26
2023-03-01 17:59:36,678 - __main__ - INFO - rescore index:2
2023-03-01 17:59:37,955 - __main__ - INFO - rescore index:1
2023-03-01 17:59:39,243 - __main__ - INFO - rescore index:1
2023-03-01 17:59:39,886 - __main__ - INFO - rescore index:1
2023-03-01 17:59:44,675 - __main__ - INFO - 

2023-03-01 17:59:44,675 - __main__ - INFO - evalWER:3750,evalWCount:7876
2023-03-01 17:59:44,675 - __main__ - INFO - batch22 || Test CER: 0.32051 || Test WER: 0.47613
2023-03-01 18:04:34,366 - __main__ - INFO - rescore index:1
2023-03-01 18:04:38,890 - __main__ - INFO - rescore index:1
2023-03-01 18:04:40,196 - __main__ - INFO - rescore index:1
2023-03-01 18:04:41,496 - __main__ - INFO - rescore index:9
2023-03-01 18:04:43,711 - __main__ - INFO - rescore index:24
2023-03-01 18:04:48,836 - __main__ - INFO - rescore index:1
2023-03-01 18:04:52,060 - __main__ - INFO - rescore index:1
2023-03-01 18:04:52,704 - __main__ - INFO - rescore index:2
2023-03-01 18:04:53,654 - __main__ - INFO - rescore index:1
2023-03-01 18:04:54,921 - __main__ - INFO - rescore index:1
2023-03-01 18:04:55,566 - __main__ - INFO - rescore index:1
2023-03-01 18:04:57,497 - __main__ - INFO - rescore index:2
2023-03-01 18:05:00,338 - __main__ - INFO - 

2023-03-01 18:05:00,339 - __main__ - INFO - evalWER:3934,evalWCount:8243
2023-03-01 18:05:00,339 - __main__ - INFO - batch23 || Test CER: 0.32115 || Test WER: 0.47725
2023-03-01 18:09:20,884 - __main__ - INFO - rescore index:1
2023-03-01 18:09:22,748 - __main__ - INFO - rescore index:1
2023-03-01 18:09:24,607 - __main__ - INFO - rescore index:22
2023-03-01 18:09:29,326 - __main__ - INFO - rescore index:1
2023-03-01 18:09:32,443 - __main__ - INFO - rescore index:3
2023-03-01 18:09:33,692 - __main__ - INFO - rescore index:1
2023-03-01 18:09:35,565 - __main__ - INFO - rescore index:1
2023-03-01 18:09:36,811 - __main__ - INFO - rescore index:1
2023-03-01 18:09:37,440 - __main__ - INFO - rescore index:1
2023-03-01 18:09:39,049 - __main__ - INFO - rescore index:2
2023-03-01 18:09:39,682 - __main__ - INFO - rescore index:8
2023-03-01 18:09:45,293 - __main__ - INFO - rescore index:1
2023-03-01 18:09:47,799 - __main__ - INFO - rescore index:3
2023-03-01 18:09:49,427 - __main__ - INFO - rescore index:2
2023-03-01 18:09:50,667 - __main__ - INFO - rescore index:11
2023-03-01 18:09:50,959 - __main__ - INFO - 

2023-03-01 18:09:50,960 - __main__ - INFO - evalWER:4110,evalWCount:8613
2023-03-01 18:09:50,960 - __main__ - INFO - batch24 || Test CER: 0.32077 || Test WER: 0.47719
2023-03-01 18:14:23,501 - __main__ - INFO - rescore index:1
2023-03-01 18:14:28,534 - __main__ - INFO - rescore index:2
2023-03-01 18:14:41,164 - __main__ - INFO - rescore index:29
2023-03-01 18:14:41,785 - __main__ - INFO - rescore index:3
2023-03-01 18:14:44,285 - __main__ - INFO - rescore index:2
2023-03-01 18:14:46,154 - __main__ - INFO - rescore index:1
2023-03-01 18:14:47,078 - __main__ - INFO - 

2023-03-01 18:14:47,078 - __main__ - INFO - evalWER:4237,evalWCount:8967
2023-03-01 18:14:47,078 - __main__ - INFO - batch25 || Test CER: 0.31693 || Test WER: 0.47251
2023-03-01 18:19:34,466 - __main__ - INFO - rescore index:1
2023-03-01 18:19:35,096 - __main__ - INFO - rescore index:11
2023-03-01 18:19:39,804 - __main__ - INFO - rescore index:2
2023-03-01 18:19:40,441 - __main__ - INFO - rescore index:1
2023-03-01 18:19:50,214 - __main__ - INFO - rescore index:4
2023-03-01 18:19:51,461 - __main__ - INFO - rescore index:1
2023-03-01 18:19:52,718 - __main__ - INFO - rescore index:1
2023-03-01 18:19:55,840 - __main__ - INFO - rescore index:2
2023-03-01 18:19:57,100 - __main__ - INFO - rescore index:1
2023-03-01 18:19:58,041 - __main__ - INFO - 

2023-03-01 18:19:58,041 - __main__ - INFO - evalWER:4397,evalWCount:9335
2023-03-01 18:19:58,041 - __main__ - INFO - batch26 || Test CER: 0.31667 || Test WER: 0.47102
2023-03-01 18:24:41,192 - __main__ - INFO - rescore index:2
2023-03-01 18:24:41,810 - __main__ - INFO - rescore index:1
2023-03-01 18:24:47,738 - __main__ - INFO - rescore index:3
2023-03-01 18:24:49,622 - __main__ - INFO - rescore index:2
2023-03-01 18:24:50,884 - __main__ - INFO - rescore index:1
2023-03-01 18:24:55,051 - __main__ - INFO - rescore index:25
2023-03-01 18:25:01,905 - __main__ - INFO - rescore index:4
2023-03-01 18:25:03,786 - __main__ - INFO - rescore index:8
2023-03-01 18:25:06,059 - __main__ - INFO - rescore index:2
2023-03-01 18:25:07,312 - __main__ - INFO - rescore index:1
2023-03-01 18:25:07,933 - __main__ - INFO - rescore index:1
2023-03-01 18:25:09,819 - __main__ - INFO - rescore index:1
2023-03-01 18:25:12,015 - __main__ - INFO - 

2023-03-01 18:25:12,015 - __main__ - INFO - evalWER:4548,evalWCount:9689
2023-03-01 18:25:12,015 - __main__ - INFO - batch27 || Test CER: 0.31455 || Test WER: 0.46940
2023-03-01 18:27:52,053 - __main__ - INFO - rescore index:3
2023-03-01 18:27:53,303 - __main__ - INFO - rescore index:2
2023-03-01 18:27:57,280 - __main__ - INFO - rescore index:3
2023-03-01 18:27:57,903 - __main__ - INFO - rescore index:1
2023-03-01 18:27:58,528 - __main__ - INFO - rescore index:1
2023-03-01 18:27:59,772 - __main__ - INFO - rescore index:2
2023-03-01 18:27:59,948 - __main__ - INFO - 

2023-03-01 18:27:59,949 - __main__ - INFO - evalWER:4640,evalWCount:9890
2023-03-01 18:27:59,949 - __main__ - INFO - batch28 || Test CER: 0.31504 || Test WER: 0.46916
2023-03-01 18:27:59,949 - __main__ - INFO - evalWER:4640,evalCCount:9890
2023-03-01 18:27:59,949 - __main__ - INFO - VOMODAL || Test CER: 0.31504 || Test WER: 0.46916
2023-03-01 18:27:59,949 - __main__ - INFO - 
Testing Done.

